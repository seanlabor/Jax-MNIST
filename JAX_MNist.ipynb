{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZuyP-M3KPUR"
      },
      "source": [
        "# MLP training on MNIST\n",
        "\n",
        "1.   Testing on all 10000 images,because faster. Problem?\n",
        "2.   Listeneintrag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "8-SzJ0NTKRP1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import jax\n",
        "import _pickle as cPickle\n",
        "import pickle\n",
        "import copy\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "import random\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from jax.example_libraries import stax, optimizers\n",
        "import torchvision\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.utils.data as data_utils\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import _pickle as cPickle\n",
        "import time\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries.stax import Dense, Relu, LogSoftmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from jax import random "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK6txN1AoYl5",
        "outputId": "c3068d53-c663-40b9-9f34-9002c8344afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjaZ90SMbWF0"
      },
      "source": [
        "### Variables\n",
        "Most variables overriden in main code part!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "juIe4Dp-bVTM"
      },
      "outputs": [],
      "source": [
        "#Needs Cleaning\n",
        "\n",
        "'''Set your file directorys'''\n",
        "file_name=\"JAX_MNist_focus\"\n",
        "googledrive_path=\"/content/drive/MyDrive/Colab Notebooks/Jax_MNist/\"\n",
        "local_path=\"C:/Users/Flo/Documents/Uni/Masterarbeit/Hanabi/Mnist handwritten digits/\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''number of training epochs for Network2'''\n",
        "n_training_epochs = 3 #number of training epochs for every NN2.  \n",
        "n_offsp_epoch = 10 #number of training and testing runs combined, to get an average for the performance of the Convu net.\n",
        "n_testing_epochs = 3 # number of testing runs, per n_offsp_epoch\n",
        "\n",
        "'''parameter Network2'''\n",
        "n_samples = 150  #Number of training samples for NN2, distribution of data for 600: [68, 59, 66, 67, 47, 46, 65, 71, 53, 58])\n",
        "n_test=1000 #Number of test samples for NN2. Needs to be multiples of batch_size_test=500 )\n",
        "batch_size = 50 # for precise n_samples number must be: n_samples%batch_size_train=0\n",
        "\n",
        "learning_rate = 0.1\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "\n",
        "\n",
        "'''Standard deviation for gaussian noise in Network1'''\n",
        "'''!!! Important hyperparameter, >=1 gives bad results'''\n",
        "std_modifier=0.05\n",
        "\n",
        "\n",
        "\n",
        "'''number of offsprings per metaepoch'''\n",
        "n_offsprings=100\n",
        "'''number of metaopochs'''\n",
        "n_metaepochs=10\n",
        "\n",
        "\n",
        "NNin1=2500 #dependent on Convu\n",
        "NNout1=10\n",
        "\n",
        "\n",
        "\n",
        "'''Convunet'''\n",
        "Convu1_in=1\n",
        "Convu2_in=12\n",
        "Convu3_in=24\n",
        "seed_convu=0\n",
        "\n",
        "n_samples=150 #number of training samples\n",
        "batch_size = 50\n",
        "n_test=1000\n",
        "n_training_epochs=3\n",
        "print_distribution_data=False\n",
        "std_modifier=0.05\n",
        "\n",
        "\n",
        "Convu1_in=32\n",
        "Convu2_in=16\n",
        "Convu3_in=4\n",
        "kernelsize_=(3,3)\n",
        "\n",
        "\n",
        "\n",
        "use_sigma_decay=True\n",
        "sigma_start=1.0\n",
        "sigma_goal=0.1\n",
        "\n",
        "'''logging to screen variables'''\n",
        "print_offsprings=True\n",
        "print_distribution_data=False\n",
        "use_sigma_decay=True #otherwise using constant sigma from config tab\n",
        "sigma_start=1 \n",
        "sigma_goal=0.05 #sigma goal after n_metaepochs\n",
        "        \n",
        "'''choose either method, softmax or elitist=keep only best offspring'''\n",
        "use_softmax=True\n",
        "temperature=0.05\n",
        "use_elitist=False\n",
        "use_winnerlist=False\n",
        "\n",
        "n_metaepochs=30 #overwriting variable from config tab, delete later\n",
        "n_offsprings=10 #overwriting variable from config tab, delete later\n",
        "\n",
        "'''number of training epochs for Network2'''\n",
        "n_offsp_epoch = 2\n",
        "n_testing_epochs = 5\n",
        "n_metaepochs=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwG6IZfAO9Y6"
      },
      "source": [
        "## **Funktions**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logg_to_file (string_, array=None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    \n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "def log_variables():\n",
        "    \n",
        "    logg_to_file ((\"n_training_epochs = {}\".format(n_training_epochs)))\n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    \n",
        "    logg_to_file ((\"n_samples = {}\".format(n_samples)))\n",
        "    logg_to_file ((\"n_test = {}\".format(n_test)))\n",
        "    logg_to_file ((\"batch_size = {}\".format(batch_size)))\n",
        "\n",
        "    logg_to_file ((\"use_focus = {}\".format(use_focus)))\n",
        "    logg_to_file ((\"focus_layer = {}\".format(focus_layer)))\n",
        "    logg_to_file ((\"focus_change_every = {}\".format(focus_change_every)))\n",
        "\n",
        "    \n",
        "    logg_to_file ((\"use_sigma_decay = {}\".format(use_sigma_decay)))\n",
        "    logg_to_file ((\"n_decay_epochs = {}\".format(n_decay_epochs)))\n",
        "    logg_to_file ((\"sigma_start = {}\".format(sigma_start)))\n",
        "    logg_to_file ((\"sigma_goal = {}\".format(sigma_goal)))\n",
        "\n",
        "  \n",
        "    logg_to_file ((\"use_KNN = {}\".format(use_KNN)))\n",
        "    logg_to_file ((\"KNN_n_neighbors = {}\".format(KNN_n_neighbors)))\n",
        "    logg_to_file ((\"KNN_top_n = {}\".format(KNN_top_n)))\n",
        "    logg_to_file ((\"n_KNN_subsprings = {}\".format(n_KNN_subsprings)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    logg_to_file ((\"std_modifier = {}\".format(std_modifier)))\n",
        "    logg_to_file ((\"use_sigma_decay = {}\".format(use_sigma_decay)))\n",
        "    logg_to_file ((\"sigma_start = {}\".format(sigma_start)))\n",
        "    logg_to_file ((\"sigma_goal = {}\".format(sigma_goal)))\n",
        "    logg_to_file ((\"n_decay_epochs = {}\".format(n_decay_epochs)))\n",
        "    logg_to_file ((\"use_pickle = {}\".format(use_pickle)))\n",
        "    logg_to_file ((\"pickle_path = {}\".format(pickle_path)))\n",
        "    logg_to_file ((\"use_father = {}\".format(use_father)))\n",
        "\n",
        "\n",
        "    logg_to_file ((\"NNin1 = {}\".format(NNin1)))\n",
        "    logg_to_file ((\"NNout1 = {}\".format(NNout1)))\n",
        "    logg_to_file ((\"Convu_in1 = {}\".format(Convu1_in)))\n",
        "    logg_to_file ((\"Convu2_in = {}\".format(Convu2_in)))\n",
        "    logg_to_file ((\"Convu3_in = {}\".format(Convu3_in)))\n",
        "\n",
        "    logg_to_file ((\"kernelsize_ = {}\".format(kernelsize_)))\n",
        "    \n",
        "    logg_to_file ((\"n_metaepochs = {}\".format(n_metaepochs)))\n",
        "    logg_to_file ((\"n_testing_epochs = {}\".format(n_testing_epochs)))         \n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    logg_to_file ((\"n_offsprings = {}\".format(n_offsprings)))\n",
        "\n",
        "    logg_to_file ((\"use_softmax = {}\".format(use_softmax)))\n",
        "    logg_to_file ((\"temperature = {}\".format(temperature)))"
      ],
      "metadata": {
        "id": "3qxJLxQ5jWwY"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "u2kUPi97tjUM"
      },
      "outputs": [],
      "source": [
        "def pathandstuff():\n",
        "\n",
        "    global save_txt\n",
        "    global base_path\n",
        "    global save_path\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        '''Save running code file to log folder'''\n",
        "        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook\n",
        "        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder\n",
        "        print(\"on local\")\n",
        "        base_path=local_path\n",
        "    elif os.path.exists(googledrive_path):\n",
        "        print(\"on google\")\n",
        "        base_path=googledrive_path\n",
        "    else:\n",
        "        raise ValueError('Please specify save path or connect to Google Drive')\n",
        "        \n",
        "    logs_path=base_path+\"Logs/\"\n",
        "    '''Set logging and temp paths'''\n",
        "    timestamp=time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    foldername=timestamp\n",
        "    save_path=os.path.join(logs_path,foldername,)\n",
        "    save_path=save_path+\"/\"\n",
        "    print(\"Log path:\",save_path)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    save_txt = os.path.join(save_path, 'Log_Jax_MNist_{}.txt'.format(foldername))\n",
        "\n",
        "\n",
        "def logg_script(file_name, save_path):\n",
        "  source=f\"/content/drive/MyDrive/Colab Notebooks/{file_name}\"\n",
        "  destination=save_path+f\"{file_name}.ipynb\"\n",
        "  shutil.copy2(source, destination)\n",
        "\n",
        "'''logging to txt and print'''\n",
        "def logg (string_, array=None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_)\n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_, array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "27fQHXuTxQE-"
      },
      "outputs": [],
      "source": [
        "train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "x = np.concatenate((train_dataset.data,test_dataset.data))\n",
        "y= np.concatenate((train_dataset.targets,test_dataset.targets))\n",
        "\n",
        "x = jnp.array(x,dtype=\"float32\").reshape(len(x), -1)\n",
        "y = jnp.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "G4NrxSVjKt8f"
      },
      "outputs": [],
      "source": [
        "def init_MLP(layer_widths, parent_key, scale=0.01):\n",
        "\n",
        "    params = []\n",
        "    keys = jax.random.split(parent_key, num=len(layer_widths)-1)\n",
        "\n",
        "    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = jax.random.split(key)\n",
        "        params.append([\n",
        "                       scale*jax.random.normal(weight_key, shape=(out_width, in_width)),\n",
        "                       scale*jax.random.normal(bias_key, shape=(out_width,))\n",
        "                       ]\n",
        "        )\n",
        "    return params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Pfln91dvWwp0"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def MLP_predict(params, x):\n",
        "\n",
        "    hidden_layers = params[:-1]\n",
        "    activation = x\n",
        "\n",
        "    for w, b in hidden_layers:\n",
        "        activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
        "\n",
        "    w_last, b_last = params[-1]\n",
        "    logits = jnp.dot(w_last, activation) + b_last\n",
        "\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "jit_MLP_predict=jit(MLP_predict)\n",
        "\n",
        "@jit\n",
        "def batched_MLP_predict(params,x):\n",
        "  return vmap(jit_MLP_predict, (None, 0))(params,x)\n",
        "  \n",
        "jit_batched_MLP_predict=jit(batched_MLP_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "k2TQ5bv8YiO2"
      },
      "outputs": [],
      "source": [
        "Convu1_in=16\n",
        "Convu2_in=24\n",
        "Convu3_in=1\n",
        "\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,kernelsize_, padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, kernelsize_, padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu3_in, kernelsize_, padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "22q4-lVbba7U"
      },
      "outputs": [],
      "source": [
        "'''After changing Convu structure test if convu out and NN in matches, set NNin1=25*25*4 to corresponding shape in error (5, 25, 25, 4) '''\n",
        "NNin1=625\n",
        "rng=jax.random.PRNGKey(1)\n",
        "\n",
        "father_weights = conv_init(rng, (batch_size,28,28,1))\n",
        "father_weights = father_weights[1]\n",
        "\n",
        "\n",
        "x_train=x[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]\n",
        "testaffe=x_train[0:5]\n",
        "imgs = conv_apply(father_weights, testaffe.reshape(-1,28,28,1))\n",
        "\n",
        "MLP_params = init_MLP([NNin1, 10], rng)\n",
        "\n",
        "pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "YQEYcSNzVeim"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "  \n",
        "    predictions = jit_batched_MLP_predict(params, imgs)\n",
        "    #print(\"predictions\",predictions.shape)\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "    \n",
        "jit_loss_fn=jit(loss_fn)\n",
        "\n",
        "@jit\n",
        "def update(params, imgs, gt_lbls, lr=0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "\n",
        "    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)\n",
        "\n",
        "jit_update=jit(update)\n",
        "\n",
        "@jit\n",
        "def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):\n",
        "\n",
        "    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,28,28,1))\n",
        "    pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis=1)\n",
        "\n",
        "    return jnp.mean(dataset_lbls == pred_classes)\n",
        "    \n",
        "jit_accuracy=jit(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "NwD7MTmKD232"
      },
      "outputs": [],
      "source": [
        "'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''\n",
        "@jit\n",
        "def train(conv_weights, imgs, lbls,MLP_params ):\n",
        "  for n in range(n_training_epochs):  \n",
        "    for i in range(jnp.shape(imgs)[0]):\n",
        "\n",
        "      gt_labels = jax.nn.one_hot(lbls[i], 10)\n",
        "      img_conv = conv_apply(conv_weights, imgs[i].reshape(-1,28,28,1))\n",
        "      loss, MLP_params = jit_update(MLP_params, img_conv.reshape(-1,NNin1), gt_labels)\n",
        "\n",
        "  return MLP_params\n",
        "  \n",
        "jit_train=jit(train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "pU1tMDY19mpI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Running for every offspring n_offsp_epoch loops to get stable acc results. \n",
        "Every loop is trained with n_samples/batch_size * batch size training epochs.\n",
        "Everything put in jit and vmap to speed up\n",
        "\n",
        "Input  \n",
        "(10, 6, 25, 28, 28, 1) x_train\n",
        "(10, 6, 25) y_train\n",
        "(10, 1000, 28, 28, 1) test_img_off\n",
        "(10, 1000) test_lbl_off\n",
        "          \n",
        "(n_offsp_epoch, n_samples/batch_size, batch size, 28, 28, 1)\n",
        "(n_offsp_epoch, n_samples/batch_size, batch size)\n",
        "(n_offsp_epoch, n_test, 28, 28, 1)\n",
        "(n_offsp_epoch, n_test)'''\n",
        "\n",
        "@jit\n",
        "def bootstrapp_offspring_MLP(key,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  \n",
        "  \n",
        "  MLP_params = init_MLP([NNin1, NNout1], key)\n",
        "  MLP_params_trained=jit_train(conv_weights, batch_affe, labelaffe,MLP_params )\n",
        " \n",
        "  \n",
        "  result=jit_accuracy(conv_weights,MLP_params_trained,test_images,test_lbls)\n",
        "  return (result)\n",
        "\n",
        "jit_bootstrapp_offspring_MLP=jit(bootstrapp_offspring_MLP)  \n",
        "\n",
        "@jit\n",
        "def vmap_bootstrapp_offspring_MLP(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  return vmap(jit_bootstrapp_offspring_MLP, ( None,None, 0,0,0,0))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)\n",
        "  \n",
        "jit_vmap_bootstrapp_offspring_MLP=jit(vmap_bootstrapp_offspring_MLP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "LuamPm0mlIB3"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings Approach 1'''\n",
        "def create_offsprings(n_offspr, fath_weights,std_modifier,seed):\n",
        "  np.random.seed(seed)\n",
        "  statedic_list=[]\n",
        "  for i in range(0,n_offspr):\n",
        "    dicta = [()] * len(father_weights)\n",
        "    for idx,w in enumerate(father_weights):\n",
        "        if w:\n",
        "            w, b = w\n",
        "            #print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))\n",
        "      \n",
        "            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''\n",
        "            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):\n",
        "              x_w=w\n",
        "              x_b=b\n",
        "            else:\n",
        "              seed=np.random.randint(0,100000)\n",
        "              key = random.PRNGKey(seed)\n",
        "              x_w = w+random.normal(key,shape=w.shape)*std_modifier #tested, random.normal adding different random noise value to every single weight\n",
        "              x_b = b+random.normal(key,shape=b.shape)*std_modifier\n",
        "            dicta[idx]=(x_w,x_b)\n",
        "    \n",
        "    statedic_list.append(dicta)\n",
        "  return statedic_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "IrbdBvjncJM_"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings Approach 2'''\n",
        "def create_focus_offsprings(n_offspr, fath_weights,std_modifier, focus_layer):\n",
        "  statedic_list=[]\n",
        "  for i in range(0,n_offspr):\n",
        "    dicta = [()] * len(father_weights)\n",
        "    for idx,w in enumerate(father_weights):\n",
        "        if w:\n",
        "            w, b = w\n",
        "            #print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))\n",
        "      \n",
        "            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''\n",
        "            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]) or idx!=focus_layer:\n",
        "              x_w=w\n",
        "              x_b=b\n",
        "            else:\n",
        "              seed=np.random.randint(0,100000)\n",
        "              key = random.PRNGKey(seed)\n",
        "              x_w = w+random.normal(key,shape=w.shape)*std_modifier #tested, random.normal adding different random noise value to every single weight\n",
        "              x_b = b+random.normal(key,shape=b.shape)*std_modifier\n",
        "            dicta[idx]=(x_w,x_b)\n",
        "    \n",
        "    statedic_list.append(dicta)\n",
        "  return statedic_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Np2DOGve_e5E"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings Approach 2, filling treeleaf of 0 and 1 with gaussian noise, doesnt seem to be a problem, ex in offspring_list[0][5]'''\n",
        "\n",
        "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
        "    if treedef is None:\n",
        "        treedef = jax.tree_structure(target)\n",
        "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
        "    return jax.tree_unflatten(treedef, keys)\n",
        "\n",
        "\n",
        "def tree_random_normal_like(rng_key, target,std_modifier):\n",
        "    keys_tree = random_split_like_tree(rng_key, target)\n",
        "    return jax.tree_multimap(\n",
        "        lambda l, k: jax.random.normal(k, l.shape, l.dtype)*std_modifier,\n",
        "        target,\n",
        "        keys_tree,\n",
        "    )\n",
        "\n",
        "def jax_create_offsprings(key,n_offspr,  fath_weights,std_modifier):\n",
        "  statedic_list=[]\n",
        "  for i in range(0,n_offspr):\n",
        "    rng=jax.random.PRNGKey(key+i)\n",
        "    random_value_tree=tree_random_normal_like(rng,fath_weights,std_modifier)\n",
        "    son=jax.tree_map(lambda x,y: x+y, fath_weights,random_value_tree)\n",
        "    statedic_list.append(son)\n",
        "\n",
        "  return statedic_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Le4KpG9snz40"
      },
      "outputs": [],
      "source": [
        "'''softmax for offspring list for approach 2\n",
        "    checked 11.04 working correctly'''\n",
        "def softmax_offlist(off_list,acc_list,temp):\n",
        "  softmax_list=softmax_result(acc_list,temp)\n",
        "  for i in range(len(off_list)):\n",
        "    if i==0:\n",
        "      top_dog=jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "    else:\n",
        "      general_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "      top_dog=jax.tree_map(lambda x,y: x+y, top_dog,general_dog)\n",
        "  return top_dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "9T8A8EkigTi8"
      },
      "outputs": [],
      "source": [
        "'''Creates softmax/temp list out of accuracy list [0.2,0.3,....,0.8]'''\n",
        "def softmax_result(results,temp: float):\n",
        "    x = [z/temp for z in results]\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def sigma_decay(start, end, n_iter):\n",
        "  return(end/start)**(1/n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "BGCe6p1BsUL2"
      },
      "outputs": [],
      "source": [
        "def KNN_weight_update(result_list_metaepoch, offspring_list):\n",
        "\n",
        "  offspring_list2=[]\n",
        "  flat_tw_weight_list=[]\n",
        "\n",
        "  '''Get weights of top x'''\n",
        "  acc_list=[x[0] for x in result_list_metaepoch]\n",
        "  sorted_list = copy.deepcopy(acc_list)\n",
        "  sorted_list.sort(reverse=True)\n",
        "  top_n=sorted_list[:Elitist_top_n]\n",
        "  index_tw=[acc_list.index(x) for x in top_n]\n",
        "  tw_weight_list=[offspring_list[i] for i in index_tw]\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  for weight in tw_weight_list:\n",
        "    affe=jax.flatten_util.ravel_pytree(weight)\n",
        "    flat_tw_weight_list.append(np.array(affe[0]))\n",
        "  knn = NearestNeighbors(n_neighbors=KNN_n_neighbors)\n",
        "  knn.fit(flat_tw_weight_list)\n",
        "  distance_mat, neighbours_mat = knn.kneighbors(flat_tw_weight_list)\n",
        "    \n",
        "  for liste in neighbours_mat:\n",
        "    weight_list=[tw_weight_list[i] for i in liste]\n",
        "    acc_list=[top_n[i] for i in liste]\n",
        "    new_offspring=softmax_offlist(weight_list,acc_list,temp)\n",
        "    offspring_list2.append(new_offspring)\n",
        "  return offspring_list2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def elitist_performer(result_list_metaepoch, offspring_list):\n",
        "  \n",
        "  offspring_list2=[]\n",
        "  '''Get weights of top x'''\n",
        "  acc_list=[x[0] for x in result_list_metaepoch]\n",
        "  sorted_list = copy.deepcopy(acc_list)\n",
        "  sorted_list.sort(reverse=True)\n",
        "  top_n=sorted_list[:Elitist_top_n]\n",
        "  index_tw=[acc_list.index(x) for x in top_n]\n",
        "  tw_weight_list=[offspring_list[i] for i in index_tw]\n",
        "  for performer in tw_weight_list:\n",
        "      new_offsprings=jax_create_offsprings((meta+numpy_seed),n_elitist_offsprings, performer,std_modifier)\n",
        "      offspring_list2.extend(new_offsprings)\n",
        "      if use_father:\n",
        "        offspring_list2.append(performer)\n",
        "  return offspring_list2\n"
      ],
      "metadata": {
        "id": "N7OhjMoFgqu1"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "bJ-Y-b84Sd2j"
      },
      "outputs": [],
      "source": [
        "'''Initialize Variables'''\n",
        "\n",
        "use_elitist=True #can be combined with Focus_Update\n",
        "Elitist_top_n=10\n",
        "n_elitist_offsprings=50\n",
        "\n",
        "n_metaepochs=100000\n",
        "n_offsprings=500\n",
        "\n",
        "n_training_epochs=20 #= how many times, is the MLP trained with the same data. Reduces dependance on the initialization of MLP weights\n",
        "\n",
        "n_samples = 100 #n of training independent training samples for MLP, samples are stratified\n",
        "batch_size = 25\n",
        "n_test=500\n",
        "n_offsp_epoch=20 #Bootstrapping, delivers more stable results for every offspring\n",
        "\n",
        "'''keys'''\n",
        "starting_key=48 #define starting point\n",
        "MLP_key=685 #seed \n",
        "numpy_seed=47 #in create offsprings\n",
        "\n",
        "use_sigma_decay=False #otherwise using constant sigma from config tab\n",
        "n_decay_epochs=int(n_metaepochs/2)   # over how many metaepochs sigma is decayed\n",
        "sigma_start=0.3\n",
        "sigma_goal=0.05 #sigma goal after n_metaepochs\n",
        "\n",
        "'''KNN weight update, disable sigma_decay, start with small sigma'''\n",
        "use_KNN=False\n",
        "KNN_n_neighbors=3\n",
        "KNN_top_n=10\n",
        "n_KNN_subsprings=50 #number offsprings of every KNN update\n",
        "\n",
        "use_Softmax=False\n",
        "\n",
        "use_focus=False\n",
        "focus_layer=[0,4,8]\n",
        "focus_change_every=100\n",
        "\n",
        "use_pickle=True #load weights\n",
        "use_best_weights=False\n",
        "pickle_path=\"/content/drive/MyDrive/Colab Notebooks/Jax_MNist/Logs/19.04.2022_2parallel/best_weight_0.8865.pkl\"\n",
        "use_father=True\n",
        "std_modifier=0.01\n",
        "temp=0.01 #weight for softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDs8dpz98msW",
        "outputId": "9f0c22da-97da-4adf-bbe4-04eb95980f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on google\n",
            "Log path: /content/drive/MyDrive/Colab Notebooks/Jax_MNist/Logs/24.04.2022_09.04/\n",
            "pickle weights imported\n",
            "\tLänge Offspring List: 500\n",
            "New best performer mean: 0.8734, std: 0.02\n",
            "New best performer mean: 0.8735, std: 0.02\n",
            "New best performer mean: 0.8737, std: 0.02\n",
            "New best performer mean: 0.8738, std: 0.02\n",
            "New best performer mean: 0.8754, std: 0.02\n",
            "New best performer mean: 0.8755, std: 0.02\n",
            "\tMetaepoch mean: 0.8725, std: 0.00\n",
            "\tMetaepoch max performer: 0.8755, min performer: 0.8675\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8705, std: 0.00\n",
            "\tMetaepoch max performer: 0.8737, min performer: 0.8664\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "New best performer mean: 0.8773, std: 0.02\n",
            "New best performer mean: 0.8787, std: 0.02\n",
            "\tMetaepoch mean: 0.8753, std: 0.00\n",
            "\tMetaepoch max performer: 0.8787, min performer: 0.8705\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8723, std: 0.00\n",
            "\tMetaepoch max performer: 0.8752, min performer: 0.8697\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8675, std: 0.00\n",
            "\tMetaepoch max performer: 0.8715, min performer: 0.8634\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8758, std: 0.00\n",
            "\tMetaepoch max performer: 0.8782, min performer: 0.8712\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8715, std: 0.00\n",
            "\tMetaepoch max performer: 0.8746, min performer: 0.8669\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8630, std: 0.00\n",
            "\tMetaepoch max performer: 0.8657, min performer: 0.8597\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "New best performer mean: 0.8793, std: 0.01\n",
            "\tMetaepoch mean: 0.8771, std: 0.00\n",
            "\tMetaepoch max performer: 0.8793, min performer: 0.8731\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8743, std: 0.00\n",
            "\tMetaepoch max performer: 0.8764, min performer: 0.8714\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8717, std: 0.00\n",
            "\tMetaepoch max performer: 0.8738, min performer: 0.8677\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8634, std: 0.00\n",
            "\tMetaepoch max performer: 0.8652, min performer: 0.8614\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8764, std: 0.00\n",
            "\tMetaepoch max performer: 0.8786, min performer: 0.8735\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8748, std: 0.00\n",
            "\tMetaepoch max performer: 0.8769, min performer: 0.8723\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8698, std: 0.00\n",
            "\tMetaepoch max performer: 0.8729, min performer: 0.8670\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8732, std: 0.00\n",
            "\tMetaepoch max performer: 0.8752, min performer: 0.8717\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8684, std: 0.00\n",
            "\tMetaepoch max performer: 0.8708, min performer: 0.8653\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8671, std: 0.00\n",
            "\tMetaepoch max performer: 0.8694, min performer: 0.8653\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8659, std: 0.00\n",
            "\tMetaepoch max performer: 0.8681, min performer: 0.8631\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8726, std: 0.00\n",
            "\tMetaepoch max performer: 0.8755, min performer: 0.8701\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8721, std: 0.00\n",
            "\tMetaepoch max performer: 0.8743, min performer: 0.8701\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8617, std: 0.00\n",
            "\tMetaepoch max performer: 0.8633, min performer: 0.8594\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8747, std: 0.00\n",
            "\tMetaepoch max performer: 0.8771, min performer: 0.8724\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "New best performer mean: 0.8804, std: 0.02\n",
            "New best performer mean: 0.8813, std: 0.02\n",
            "\tMetaepoch mean: 0.8793, std: 0.00\n",
            "\tMetaepoch max performer: 0.8813, min performer: 0.8771\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8652, std: 0.00\n",
            "\tMetaepoch max performer: 0.8668, min performer: 0.8622\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8715, std: 0.00\n",
            "\tMetaepoch max performer: 0.8736, min performer: 0.8679\n",
            "\tTime per metaepoch:16.3s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8682, std: 0.00\n",
            "\tMetaepoch max performer: 0.8701, min performer: 0.8663\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8736, std: 0.00\n",
            "\tMetaepoch max performer: 0.8754, min performer: 0.8713\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8666, std: 0.00\n",
            "\tMetaepoch max performer: 0.8688, min performer: 0.8650\n",
            "\tTime per metaepoch:16.2s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8706, std: 0.00\n",
            "\tMetaepoch max performer: 0.8726, min performer: 0.8686\n",
            "\tTime per metaepoch:16.1s\n",
            "\n",
            "\tLänge Offspring List: 510\n",
            "\tMetaepoch mean: 0.8678, std: 0.00\n",
            "\tMetaepoch max performer: 0.8698, min performer: 0.8645\n",
            "\tTime per metaepoch:16.0s\n",
            "\n",
            "\tLänge Offspring List: 510\n"
          ]
        }
      ],
      "source": [
        "from math import e\n",
        "#main code\n",
        "'''Initialize variables'''\n",
        "f_idx=0\n",
        "focus_layer=focus_layer*100\n",
        "rng_MLP=jax.random.PRNGKey(MLP_key)\n",
        "results_meta=[]\n",
        "best_performer=[0.0,0.0]\n",
        "father_key=jax.random.PRNGKey(starting_key)\n",
        "best_weights=conv_init(father_key, (batch_size,28,28,1))[1]\n",
        "\n",
        "'''Start Logging'''\n",
        "pathandstuff()\n",
        "logg_script(file_name, save_path)\n",
        "log_variables()\n",
        "\n",
        "for meta in range (n_metaepochs):\n",
        "    start_meta = time.time()\n",
        "\n",
        "    '''Sigma Decay'''\n",
        "    if use_sigma_decay:\n",
        "        sigma_base=sigma_decay(sigma_start, sigma_goal, n_decay_epochs)\n",
        "        if meta < n_decay_epochs:\n",
        "          std_modifier=sigma_start*sigma_base**meta\n",
        "        else:\n",
        "          std_modifier=sigma_start*sigma_base**n_decay_epochs\n",
        "\n",
        "\n",
        "    '''Starting point'''\n",
        "    if meta ==0:\n",
        "        if use_pickle:\n",
        "            with open(pickle_path, \"rb\") as input_file:\n",
        "              father_weights = cPickle.load(input_file)\n",
        "              print(\"pickle weights imported\") \n",
        "            offspring_list=jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "            if use_father:\n",
        "              offspring_list[0]=father_weights\n",
        "        else:\n",
        "          father_weights = conv_init(father_key, (batch_size,28,28,1))\n",
        "          father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "          offspring_list=jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "          if use_father:\n",
        "            offspring_list[0]=father_weights\n",
        "\n",
        "    '''Weight updates'''      \n",
        "    if meta >=1:\n",
        "\n",
        "        '''KNN weight update'''\n",
        "        if use_KNN:\n",
        "          KNN_offlist=KNN_weight_update(result_list_metaepoch, offspring_list)\n",
        "          offspring_list=[]\n",
        "          if use_father:\n",
        "            offspring_list.append(best_weights)\n",
        "          for off in KNN_offlist:\n",
        "            offspring_list.append(off)\n",
        "            offspring_list.extend(jax_create_offsprings((meta+numpy_seed),n_KNN_subsprings, father_weights,std_modifier))\n",
        "          \n",
        "\n",
        "        '''Softmax Update'''\n",
        "        if use_Softmax:\n",
        "          grand_father=offspring_list[0]\n",
        "          father_weights=softmax_offlist(offspring_list,[x[0] for x in result_list_metaepoch],temp)\n",
        "          offspring_list=jax_create_offsprings((meta+numpy_seed),n_offsprings, father_weights,std_modifier)\n",
        "          if use_father:\n",
        "            offspring_list[0]=grand_father\n",
        "            offspring_list[1]=best_weights\n",
        "            offspring_list[2]=father_weights\n",
        "        \n",
        "        '''Focus Weight Update'''\n",
        "        if use_focus:\n",
        "          grand_father=offspring_list[0]\n",
        "          if meta % focus_change_every ==0:\n",
        "            f_idx=f_idx+1\n",
        "            logg(f\"Focus change to layer {focus_layer[f_idx]}\")\n",
        "          if use_elitist:\n",
        "            offspring_list=elitist_performer(result_list_metaepoch, offspring_list)\n",
        "          else: \n",
        "            father_weights=softmax_offlist(offspring_list,[x[0] for x in result_list_metaepoch],temp)\n",
        "            offspring_list=create_focus_offsprings(n_offsprings, father_weights,std_modifier, focus_layer[f_idx])\n",
        "          if use_father:\n",
        "            offspring_list[0]=grand_father\n",
        "            offspring_list[1]=best_weights\n",
        "            offspring_list[2]=father_weights\n",
        "\n",
        "        if use_elitist:\n",
        "          offspring_list=elitist_performer(result_list_metaepoch, offspring_list)\n",
        "          \n",
        "    result_list_metaepoch=[]\n",
        "\n",
        "    '''same data for every offspring'''\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=n_offsp_epoch*n_samples,\n",
        "                                                          test_size=n_offsp_epoch*n_test,stratify=y,\n",
        "                                                          random_state=(starting_key+meta))\n",
        "    \n",
        "    x_train=x_train.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)\n",
        "    y_train=y_train.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size)\n",
        "    x_test=x_test.reshape(n_offsp_epoch,n_test,28,28,1)\n",
        "    y_test=y_test.reshape(n_offsp_epoch,n_test)\n",
        "    \n",
        "    print(\"\\tLänge Offspring List:\",len(offspring_list))\n",
        "    #print(f\"\\tTime overhead: {(time.time()-start_meta):.2f}s\")\n",
        "\n",
        "    for i in range(len(offspring_list)):\n",
        "\n",
        "      conv_weights=offspring_list[i]\n",
        "      result_off=jit_vmap_bootstrapp_offspring_MLP(rng_MLP,conv_weights,x_train,y_train,x_test,y_test)\n",
        "      result_off2=[float(jnp.mean(result_off)),float(jnp.std(result_off))]\n",
        "      result_list_metaepoch.append(result_off2)\n",
        "\n",
        "\n",
        "      '''Check for best performer'''\n",
        "      if result_off2[0]>best_performer[0]:\n",
        "        best_performer=result_off2\n",
        "        best_weights=conv_weights\n",
        "        with open(save_path+f\"best_weight_{result_off2[0]:.4f}.pkl\", 'wb') as f:\n",
        "          pickle.dump(best_weights, f, pickle.HIGHEST_PROTOCOL)\n",
        "          f.close()\n",
        "        logg(f\"New best performer mean: {best_performer[0]:.4f}, std: {best_performer[1]:.2f}\")\n",
        "      \n",
        "    logg(\"\\tMetaepoch mean: {:.4f}, std: {:.2f}\".format(np.mean(np.array([x[0] for x in result_list_metaepoch])),np.std(np.array([x[0] for x in result_list_metaepoch]))))\n",
        "    logg(\"\\tMetaepoch max performer: {:.4f}, min performer: {:.4f}\".format(np.max(np.array([x[0] for x in result_list_metaepoch])),np.min(np.array([x[0] for x in result_list_metaepoch]))))\n",
        "    logg(\"\\tTime per metaepoch:{:.1f}s\\n\".format(time.time() - start_meta))\n",
        "    results_meta.append(np.mean(np.array(result_list_metaepoch), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(offspring_list[2][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avsFw4z_y-RM",
        "outputId": "ef921ee0-fd95-4a29-b566-4ecae08e39aa"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-LoVsaKOzqf"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6h21gTzssZg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kamZwkFSwK0x"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fat9dd-ywK3r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17Bxhu24pBJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOx7LJcv4pEA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0fuAVox4pG0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sh6SGdg4pJu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeNKQkLO4pMi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTmFie-V4pPR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qxo6Fzs4pRu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnOeDl_bnyii"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv40L_gRnylW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzCZjC1dnyoQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMmOX-VSKTjQ"
      },
      "source": [
        "# Archiv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "JAX_MNist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}