{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZuyP-M3KPUR"
      },
      "source": [
        "# MLP training on MNIST\n",
        "\n",
        "1.   Testing on all 10000 images,because faster. Problem?\n",
        "2.   Listeneintrag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8-SzJ0NTKRP1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import jax\n",
        "import pickle\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "import random\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from jax.example_libraries import stax, optimizers\n",
        "import torchvision\n",
        "import torch\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch.utils.data as data_utils\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import _pickle as cPickle\n",
        "import time\n",
        "from jax.example_libraries import stax\n",
        "from jax.example_libraries.stax import Dense, Relu, LogSoftmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from jax import random "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SK6txN1AoYl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b42a656-69a1-4c54-bf5a-9edb2f02217b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjaZ90SMbWF0"
      },
      "source": [
        "### Variables\n",
        "Most variables overriden in main code part!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "juIe4Dp-bVTM"
      },
      "outputs": [],
      "source": [
        "#Needs Cleaning\n",
        "\n",
        "'''Set your file directorys'''\n",
        "file_name=\"JAX_MNist\"\n",
        "googledrive_path=\"/content/drive/MyDrive/Colab Notebooks/Jax_MNist/\"\n",
        "local_path=\"C:/Users/Flo/Documents/Uni/Masterarbeit/Hanabi/Mnist handwritten digits/\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''number of training epochs for Network2'''\n",
        "n_training_epochs = 3 #number of training epochs for every NN2.  \n",
        "n_offsp_epoch = 10 #number of training and testing runs combined, to get an average for the performance of the Convu net.\n",
        "n_testing_epochs = 3 # number of testing runs, per n_offsp_epoch\n",
        "\n",
        "'''parameter Network2'''\n",
        "n_samples = 150  #Number of training samples for NN2, distribution of data for 600: [68, 59, 66, 67, 47, 46, 65, 71, 53, 58])\n",
        "n_test=1000 #Number of test samples for NN2. Needs to be multiples of batch_size_test=500 )\n",
        "batch_size = 50 # for precise n_samples number must be: n_samples%batch_size_train=0\n",
        "\n",
        "learning_rate = 0.1\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "\n",
        "\n",
        "'''Standard deviation for gaussian noise in Network1'''\n",
        "'''!!! Important hyperparameter, >=1 gives bad results'''\n",
        "std_modifier=0.05\n",
        "\n",
        "\n",
        "\n",
        "'''number of offsprings per metaepoch'''\n",
        "n_offsprings=100\n",
        "'''number of metaopochs'''\n",
        "n_metaepochs=10\n",
        "\n",
        "\n",
        "NNin1=2500 #dependent on Convu\n",
        "NNout1=10\n",
        "\n",
        "\n",
        "\n",
        "'''Convunet'''\n",
        "Convu1_in=1\n",
        "Convu2_in=12\n",
        "Convu3_in=24\n",
        "seed_convu=0\n",
        "\n",
        "n_samples=150 #number of training samples\n",
        "batch_size = 50\n",
        "n_test=1000\n",
        "n_training_epochs=3\n",
        "print_distribution_data=False\n",
        "std_modifier=0.05\n",
        "\n",
        "\n",
        "Convu1_in=32\n",
        "Convu2_in=16\n",
        "Convu3_in=4\n",
        "\n",
        "\n",
        "\n",
        "use_sigma_decay=True\n",
        "sigma_start=1.0\n",
        "sigma_goal=0.1\n",
        "\n",
        "'''logging to screen variables'''\n",
        "print_offsprings=True\n",
        "print_distribution_data=False\n",
        "use_sigma_decay=True #otherwise using constant sigma from config tab\n",
        "sigma_start=1 \n",
        "sigma_goal=0.05 #sigma goal after n_metaepochs\n",
        "        \n",
        "'''choose either method, softmax or elitist=keep only best offspring'''\n",
        "use_softmax=True\n",
        "temperature=0.05\n",
        "use_elitist=False\n",
        "use_winnerlist=False\n",
        "\n",
        "n_metaepochs=30 #overwriting variable from config tab, delete later\n",
        "n_offsprings=10 #overwriting variable from config tab, delete later\n",
        "\n",
        "'''number of training epochs for Network2'''\n",
        "n_offsp_epoch = 2\n",
        "n_testing_epochs = 5\n",
        "n_metaepochs=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwG6IZfAO9Y6"
      },
      "source": [
        "## **Funktions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u2kUPi97tjUM"
      },
      "outputs": [],
      "source": [
        "def pathandstuff():\n",
        "\n",
        "    global save_txt\n",
        "    global base_path\n",
        "    global save_path\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        '''Save running code file to log folder'''\n",
        "        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook\n",
        "        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder\n",
        "        print(\"on local\")\n",
        "        base_path=local_path\n",
        "    elif os.path.exists(googledrive_path):\n",
        "        print(\"on google\")\n",
        "        base_path=googledrive_path\n",
        "    else:\n",
        "        raise ValueError('Please specify save path or connect to Google Drive')\n",
        "        \n",
        "    logs_path=base_path+\"Logs/\"\n",
        "    '''Set logging and temp paths'''\n",
        "    timestamp=time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    foldername=timestamp\n",
        "    save_path=os.path.join(logs_path,foldername,)\n",
        "    save_path=save_path+\"/\"\n",
        "    print(\"Log path:\",save_path)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    save_txt = os.path.join(save_path, 'Log_Jax_MNist_{}.txt'.format(foldername))\n",
        "\n",
        "\n",
        "def logg_script(file_name, save_path):\n",
        "  source=f\"/content/drive/MyDrive/Colab Notebooks/{file_name}.ipynb\"\n",
        "  destination=save_path+f\"{file_name}.ipynb\"\n",
        "  shutil.copy2(source, destination)\n",
        "\n",
        "'''logging to txt and print'''\n",
        "def logg (string_, array=None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_)\n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    print(string_, array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "27fQHXuTxQE-"
      },
      "outputs": [],
      "source": [
        "train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "x = np.concatenate((train_dataset.data,test_dataset.data))\n",
        "y= np.concatenate((train_dataset.targets,test_dataset.targets))\n",
        "\n",
        "x = jnp.array(x,dtype=\"float32\").reshape(len(x), -1)\n",
        "y = jnp.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G4NrxSVjKt8f"
      },
      "outputs": [],
      "source": [
        "def init_MLP(layer_widths, parent_key, scale=0.01):\n",
        "\n",
        "    params = []\n",
        "    keys = jax.random.split(parent_key, num=len(layer_widths)-1)\n",
        "\n",
        "    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = jax.random.split(key)\n",
        "        params.append([\n",
        "                       scale*jax.random.normal(weight_key, shape=(out_width, in_width)),\n",
        "                       scale*jax.random.normal(bias_key, shape=(out_width,))\n",
        "                       ]\n",
        "        )\n",
        "    return params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pfln91dvWwp0"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def MLP_predict(params, x):\n",
        "\n",
        "    hidden_layers = params[:-1]\n",
        "    activation = x\n",
        "\n",
        "    for w, b in hidden_layers:\n",
        "        activation = jax.nn.relu(jnp.dot(w, activation) + b)\n",
        "\n",
        "    w_last, b_last = params[-1]\n",
        "    logits = jnp.dot(w_last, activation) + b_last\n",
        "\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "jit_MLP_predict=jit(MLP_predict)\n",
        "\n",
        "@jit\n",
        "def batched_MLP_predict(params,x):\n",
        "  return vmap(jit_MLP_predict, (None, 0))(params,x)\n",
        "  \n",
        "jit_batched_MLP_predict=jit(batched_MLP_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k2TQ5bv8YiO2"
      },
      "outputs": [],
      "source": [
        "Convu1_in=16\n",
        "Convu2_in=24\n",
        "Convu3_in=1\n",
        "\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,(3,3), padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, (3,3), padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu3_in, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "22q4-lVbba7U"
      },
      "outputs": [],
      "source": [
        "'''After changing Convu structure test if convu out and NN in matches, set NNin1=25*25*4 to corresponding shape in error (5, 25, 25, 4) '''\n",
        "NNin1=625\n",
        "rng=jax.random.PRNGKey(1)\n",
        "\n",
        "father_weights = conv_init(rng, (batch_size,28,28,1))\n",
        "father_weights = father_weights[1]\n",
        "\n",
        "\n",
        "x_train=x[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]\n",
        "testaffe=x_train[0:5]\n",
        "imgs = conv_apply(father_weights, testaffe.reshape(-1,28,28,1))\n",
        "\n",
        "MLP_params = init_MLP([NNin1, 10], rng)\n",
        "\n",
        "pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YQEYcSNzVeim"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "  \n",
        "    predictions = jit_batched_MLP_predict(params, imgs)\n",
        "    #print(\"predictions\",predictions.shape)\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "    \n",
        "jit_loss_fn=jit(loss_fn)\n",
        "\n",
        "@jit\n",
        "def update(params, imgs, gt_lbls, lr=0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "\n",
        "    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)\n",
        "\n",
        "jit_update=jit(update)\n",
        "\n",
        "@jit\n",
        "def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):\n",
        "\n",
        "    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,28,28,1))\n",
        "    pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,NNin1)), axis=1)\n",
        "\n",
        "    return jnp.mean(dataset_lbls == pred_classes)\n",
        "    \n",
        "jit_accuracy=jit(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NwD7MTmKD232"
      },
      "outputs": [],
      "source": [
        "'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''\n",
        "@jit\n",
        "def train(conv_weights, imgs, lbls,MLP_params ):\n",
        "  for n in range(n_training_epochs):  \n",
        "    for i in range(jnp.shape(imgs)[0]):\n",
        "\n",
        "      gt_labels = jax.nn.one_hot(lbls[i], 10)\n",
        "      img_conv = conv_apply(conv_weights, imgs[i].reshape(-1,28,28,1))\n",
        "      loss, MLP_params = jit_update(MLP_params, img_conv.reshape(-1,NNin1), gt_labels)\n",
        "\n",
        "  return MLP_params\n",
        "  \n",
        "jit_train=jit(train)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pU1tMDY19mpI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Running for every offspring n_offsp_epoch loops to get stable acc results. \n",
        "Every loop is trained with n_samples/batch_size * batch size training epochs.\n",
        "Everything put in jit and vmap to speed up\n",
        "\n",
        "Input  \n",
        "(10, 6, 25, 28, 28, 1) x_train\n",
        "(10, 6, 25) y_train\n",
        "(10, 1000, 28, 28, 1) test_img_off\n",
        "(10, 1000) test_lbl_off\n",
        "          \n",
        "(n_offsp_epoch, n_samples/batch_size, batch size, 28, 28, 1)\n",
        "(n_offsp_epoch, n_samples/batch_size, batch size)\n",
        "(n_offsp_epoch, n_test, 28, 28, 1)\n",
        "(n_offsp_epoch, n_test)'''\n",
        "\n",
        "@jit\n",
        "def bootstrapp_offspring_MLP(key,conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  \n",
        "  \n",
        "  MLP_params = init_MLP([NNin1, NNout1], key)\n",
        "  MLP_params_trained=jit_train(conv_weights, batch_affe, labelaffe,MLP_params )\n",
        " \n",
        "  \n",
        "  result=jit_accuracy(father_weights,MLP_params_trained,test_images,test_lbls)\n",
        "  return (result)\n",
        "\n",
        "jit_bootstrapp_offspring_MLP=jit(bootstrapp_offspring_MLP)  \n",
        "\n",
        "@jit\n",
        "def vmap_bootstrapp_offspring_MLP(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):\n",
        "  return vmap(jit_bootstrapp_offspring_MLP, ( None,None, 0,0,0,0))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)\n",
        "  \n",
        "jit_vmap_bootstrapp_offspring_MLP=jit(vmap_bootstrapp_offspring_MLP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LuamPm0mlIB3"
      },
      "outputs": [],
      "source": [
        "'''creating offsprings Approach 2'''\n",
        "def create_offsprings(n_offspr, fath_weights,std_modifier,seed):\n",
        "  np.random.seed(seed)\n",
        "  statedic_list=[]\n",
        "  for i in range(0,n_offspr):\n",
        "    dicta = [()] * len(father_weights)\n",
        "    for idx,w in enumerate(father_weights):\n",
        "        if w:\n",
        "            w, b = w\n",
        "            #print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))\n",
        "      \n",
        "            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''\n",
        "            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):\n",
        "              x_w=w\n",
        "              x_b=b\n",
        "            else:\n",
        "              seed=np.random.randint(0,100000)\n",
        "              key = random.PRNGKey(seed)\n",
        "              x_w = w+random.normal(key,shape=w.shape)*std_modifier #tested, random.normal adding different random noise value to every single weight\n",
        "              x_b = b+random.normal(key,shape=b.shape)*std_modifier\n",
        "            dicta[idx]=(x_w,x_b)\n",
        "    \n",
        "    statedic_list.append(dicta)\n",
        "  return statedic_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Le4KpG9snz40"
      },
      "outputs": [],
      "source": [
        "'''softmax for offspring list for approach 2\n",
        "    checked 11.04 working correctly'''\n",
        "def softmax_offlist(off_list,acc_list,temp):\n",
        "  softmax_list=softmax_result(acc_list,temp)\n",
        "  for i in range(len(off_list)):\n",
        "    if i==0:\n",
        "      top_dog=jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "    else:\n",
        "      general_dog = jax.tree_map(lambda x: x*softmax_list[i], off_list[i])\n",
        "      top_dog=jax.tree_map(lambda x,y: x+y, top_dog,general_dog)\n",
        "  return top_dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9T8A8EkigTi8"
      },
      "outputs": [],
      "source": [
        "'''Creates softmax/temp list out of accuracy list [0.2,0.3,....,0.8]'''\n",
        "def softmax_result(results,temp: float):\n",
        "    x = [z/temp for z in results]\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def sigma_decay(start, end, n_iter):\n",
        "  return(end/start)**(1/n_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BGCe6p1BsUL2"
      },
      "outputs": [],
      "source": [
        "def KNN_weight_update(result_list_metaepoch, offspring_list):\n",
        "\n",
        "  '''acc_list=[x[0] for x in result_list_metaepoch]\n",
        "  max_value=np.max(acc_list)\n",
        "  top_twenty=[x[0] for x in result_list_metaepoch if x[0] > max_value*(1-KNN_quantil)]\n",
        "  index_tw=[acc_list.index(x) for x in top_twenty]'''\n",
        "\n",
        "  acc_list=[x[0] for x in result_list_metaepoch]\n",
        "  acc_list.sort(reverse=True)\n",
        "  top_n=acc_list[:KNN_top_n]\n",
        "  index_tw=[acc_list.index(x) for x in acc_list[:KNN_top_n]]\n",
        "\n",
        "\n",
        "  '''Get weights of top twenty'''\n",
        "  tw_weight_list=[offspring_list[i] for i in index_tw]\n",
        "  flat_tw_weight_list=[]\n",
        "  for weight in tw_weight_list:\n",
        "    affe=jax.flatten_util.ravel_pytree(weight)\n",
        "    flat_tw_weight_list.append(np.array(affe[0]))\n",
        "  '''given condition would throw an error, return original list in this case'''\n",
        "  if len(flat_tw_weight_list) < KNN_n_neighbors:\n",
        "    KNN_use=False\n",
        "    return KNN_use,offspring_list\n",
        "  else:\n",
        "    knn = NearestNeighbors(n_neighbors=KNN_n_neighbors)\n",
        "    knn.fit(flat_tw_weight_list)\n",
        "    distance_mat, neighbours_mat = knn.kneighbors(flat_tw_weight_list)\n",
        "    offspring_list2=[]\n",
        "    for liste in neighbours_mat:\n",
        "      weight_list=[tw_weight_list[i] for i in liste]\n",
        "      acc_list=[top_n[i] for i in liste]\n",
        "      new_offspring=softmax_offlist(weight_list,acc_list,temp)\n",
        "      offspring_list2.append(new_offspring)\n",
        "      KNN_use=True\n",
        "    return KNN_use,offspring_list2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bJ-Y-b84Sd2j"
      },
      "outputs": [],
      "source": [
        "'''Initialize Variables'''\n",
        "import _pickle as cPickle\n",
        "\n",
        "n_training_epochs=20 #= how many times, is the MLP trained with the same data. Reduces dependance on the initialization of MLP weights\n",
        "\n",
        "n_samples = 100 #n of training independent training samples for MLP, samples are stratified\n",
        "batch_size = 25\n",
        "n_test=500\n",
        "n_offsp_epoch=20 #Bootstrapping, delivers more stable results for every offspring\n",
        "\n",
        "'''keys'''\n",
        "starting_key=48 #define starting point\n",
        "MLP_key=685 #seed \n",
        "numpy_seed=47 #in create offsprings\n",
        "\n",
        "use_sigma_decay=False #otherwise using constant sigma from config tab\n",
        "n_decay_epochs=int(n_metaepochs/2)   # over how many metaepochs sigma is decayed\n",
        "sigma_start=0.3\n",
        "sigma_goal=0.05 #sigma goal after n_metaepochs\n",
        "\n",
        "'''KNN weight update, disable sigma_decay, start with small sigma'''\n",
        "use_KNN=False\n",
        "KNN_n_neighbors=10\n",
        "KNN_quantil=0.10\n",
        "KNN_top_n=10\n",
        "n_KNN_subsprings=50 #number offsprings of every KNN update\n",
        "\n",
        "use_pickle=True #load weights\n",
        "pickle_path=\"/content/drive/MyDrive/Colab Notebooks/Jax_MNist/Logs/09.04.2022_09.46/best_weight.pkl\"\n",
        "use_father=True \n",
        "std_modifier=0.05\n",
        "temp=0.1 #weight for softmax\n",
        "\n",
        "n_metaepochs=1000\n",
        "n_offsprings=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDs8dpz98msW",
        "outputId": "c1683d3d-cb08-444a-99a2-603b6e68ec6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on google\n",
            "Log path: /content/drive/MyDrive/Colab Notebooks/Jax_MNist/Logs/11.04.2022_17.57/\n",
            "New best performer mean: 0.7099, std: 0.04\n",
            "\tMetaepoch mean: 0.6966, std: 0.01\n",
            "\tMetaepoch max performer: 0.7099, min performer: 0.6578\n",
            "\tTime per metaepoch:46.4s\n",
            "\n",
            "\tMetaepoch mean: 0.6957, std: 0.01\n",
            "\tMetaepoch max performer: 0.7086, min performer: 0.6647\n",
            "\tTime per metaepoch:31.9s\n",
            "\n",
            "\tMetaepoch mean: 0.6951, std: 0.01\n",
            "\tMetaepoch max performer: 0.7073, min performer: 0.6618\n",
            "\tTime per metaepoch:31.6s\n",
            "\n",
            "\tMetaepoch mean: 0.6940, std: 0.01\n",
            "\tMetaepoch max performer: 0.7053, min performer: 0.6590\n",
            "\tTime per metaepoch:31.6s\n",
            "\n",
            "\tMetaepoch mean: 0.6844, std: 0.01\n",
            "\tMetaepoch max performer: 0.7000, min performer: 0.6476\n",
            "\tTime per metaepoch:31.9s\n",
            "\n",
            "New best performer mean: 0.7106, std: 0.03\n",
            "\tMetaepoch mean: 0.6978, std: 0.01\n",
            "\tMetaepoch max performer: 0.7106, min performer: 0.6581\n",
            "\tTime per metaepoch:32.2s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from math import e\n",
        "#main code\n",
        "'''Start Logging'''\n",
        "pathandstuff()\n",
        "logg_script(file_name, save_path)\n",
        "\n",
        "rng_MLP=jax.random.PRNGKey(MLP_key)\n",
        "results_meta=[]\n",
        "best_performer=[0.0,0.0]\n",
        "father_key=jax.random.PRNGKey(starting_key)\n",
        "best_weights=conv_init(father_key, (batch_size,28,28,1))[1]\n",
        "\n",
        "for meta in range (n_metaepochs):\n",
        "    start_meta = time.time()\n",
        "    if use_sigma_decay:\n",
        "        sigma_base=sigma_decay(sigma_start, sigma_goal, n_decay_epochs)\n",
        "        if meta < n_decay_epochs:\n",
        "          std_modifier=sigma_start*sigma_base**meta\n",
        "        else:\n",
        "          std_modifier=sigma_start*sigma_base**n_decay_epochs\n",
        "\n",
        "    if meta ==0:\n",
        "        if use_pickle:\n",
        "            #father_weights = best_weights\n",
        "            with open(pickle_path, \"rb\") as input_file:\n",
        "              father_weights = cPickle.load(input_file)\n",
        "              print(\"weights imported\")\n",
        "            offspring_list=create_offsprings(n_offsprings, father_weights,std_modifier,(meta+numpy_seed))\n",
        "            if use_father:\n",
        "              offspring_list[0]=father_weights\n",
        "        else:\n",
        "          father_weights = conv_init(father_key, (batch_size,28,28,1))\n",
        "          father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "          offspring_list=create_offsprings(n_offsprings, father_weights,std_modifier,(meta+numpy_seed))\n",
        "          if use_father:\n",
        "            offspring_list[0]=father_weights\n",
        "          \n",
        "    if meta >=1:\n",
        "        '''KNN weight update'''\n",
        "        if use_KNN:\n",
        "          KNN_used, KNN_offlist=KNN_weight_update(result_list_metaepoch, offspring_list)\n",
        "          '''get offsprings for every KNN update'''\n",
        "          if KNN_used:\n",
        "            #print(\"KNN_offlist length\", len(KNN_offlist))\n",
        "            offspring_list=best_weights\n",
        "            for off in KNN_offlist:\n",
        "              offspring_list.append(off)\n",
        "              offspring_list.extend(create_offsprings(n_KNN_subsprings, off,std_modifier,(meta+numpy_seed)))\n",
        "          else:\n",
        "              offspring_list=KNN_offlist\n",
        "\n",
        "\n",
        "        if not use_KNN:\n",
        "          grand_father=offspring_list[0]\n",
        "          father_weights=softmax_offlist(offspring_list,[x[0] for x in result_list_metaepoch],temp)\n",
        "          offspring_list=create_offsprings(n_offsprings, father_weights,std_modifier,(meta+numpy_seed))\n",
        "          if use_father:\n",
        "            offspring_list[0]=grand_father\n",
        "            offspring_list[1]=best_weights\n",
        "            offspring_list[2]=father_weights\n",
        "        \n",
        "\n",
        "    result_list_metaepoch=[]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=n_offsp_epoch*n_samples,\n",
        "                                                          test_size=n_offsp_epoch*n_test,stratify=y,\n",
        "                                                          random_state=(starting_key+meta))\n",
        "    #print(\"\\tLänge Offspring List:\",len(offspring_list))\n",
        "    #print(\"Time overhead:\",time.time()-start_meta) \n",
        "    for i in range(len(offspring_list)):\n",
        "      \n",
        "      conv_weights=offspring_list[i]\n",
        "      \n",
        "      \n",
        "      '''same data for every offspring'''\n",
        "     \n",
        "      \n",
        "      x_train=x_train.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)\n",
        "      y_train=y_train.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size)\n",
        "\n",
        "      x_test=x_test.reshape(n_offsp_epoch,n_test,28,28,1)\n",
        "      y_test=y_test.reshape(n_offsp_epoch,n_test)\n",
        "\n",
        "      #print(jnp.shape(x_train))\n",
        "      #print(jnp.shape(y_train))\n",
        "      #print(jnp.shape(x_test))\n",
        "      #print(jnp.shape(y_test))\n",
        "\n",
        "      result_off=jit_vmap_bootstrapp_offspring_MLP(rng_MLP,conv_weights,x_train,y_train,x_test,y_test)\n",
        "      result_list_metaepoch.append([float(jnp.mean(result_off)),float(jnp.std(result_off))])\n",
        "\n",
        "    '''Check for best performer'''\n",
        "    if  np.max(result_list_metaepoch, axis=0)[0]>best_performer[0]:\n",
        "      best_performer=np.max(result_list_metaepoch, axis=0)\n",
        "      best_weights=conv_weights\n",
        "      with open(save_path+\"best_weight.pkl\", 'wb') as f:\n",
        "        pickle.dump(best_weights, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "      logg(f\"New best performer mean: {best_performer[0]:.4f}, std: {best_performer[1]:.2f}\")\n",
        "      \n",
        "    \n",
        "    logg(\"\\tMetaepoch mean: {:.4f}, std: {:.2f}\".format(np.mean(np.array([x[0] for x in result_list_metaepoch])),np.std(np.array([x[0] for x in result_list_metaepoch]))))\n",
        "    logg(\"\\tMetaepoch max performer: {:.4f}, min performer: {:.4f}\".format(np.max(np.array([x[0] for x in result_list_metaepoch])),np.min(np.array([x[0] for x in result_list_metaepoch]))))\n",
        "    logg(\"\\tTime per metaepoch:{:.1f}s\\n\".format(time.time() - start_meta))\n",
        "    results_meta.append(np.mean(np.array(result_list_metaepoch), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff_help\n"
      ],
      "metadata": {
        "id": "8io9d0w6QxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-LoVsaKOzqf"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ARF8-UcwKyC"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/MNist/best_weight.pkl\", 'wb') as f:\n",
        "\n",
        "        pickle.dump(best_weights, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6h21gTzssZg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kamZwkFSwK0x"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fat9dd-ywK3r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83S48KuI4o-O"
      },
      "outputs": [],
      "source": [
        "\n",
        "dicta = [()] * len(father_weights)\n",
        "for idx,w in enumerate(father_weights):\n",
        "    if w:\n",
        "        w, b = w\n",
        "        #print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))\n",
        "  \n",
        "        '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''\n",
        "        if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):\n",
        "          x_w=w\n",
        "          x_b=b\n",
        "        else:\n",
        "          seed=np.random.randint(0,100000)\n",
        "          key = random.PRNGKey(seed)\n",
        "          x_w = w+random.normal(key,shape=w.shape)*std_modifier\n",
        "          x_b = b+random.normal(key,shape=b.shape)*std_modifier\n",
        "          print(\"\\n\\n\\n\")\n",
        "          print(\"diff x_w\",jax.tree_map(lambda x,y: x-y, x_w,w))\n",
        "          print(\"\\n\\n\\n\")\n",
        "          print(\"diff x_b\",jax.tree_map(lambda x,y: x-y, x_b,b))\n",
        "          print(\"\\n\\n\\n\")\n",
        "        dicta[idx]=(x_w,x_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17Bxhu24pBJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOx7LJcv4pEA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0fuAVox4pG0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sh6SGdg4pJu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeNKQkLO4pMi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTmFie-V4pPR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qxo6Fzs4pRu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtXdcAsonyf_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "words = list(np.array(y_test))\n",
        "\n",
        "print(Counter(words).keys()) # equals to list(set(words))\n",
        "print(Counter(words).values()) # counts the elements' frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnOeDl_bnyii"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv40L_gRnylW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzCZjC1dnyoQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTjWvLO_kQ9Z"
      },
      "outputs": [],
      "source": [
        "  import os\n",
        "  path=os.path.join(\"/content/long_run_fatherweights_04.04.22.pkl\")\n",
        "  with open(path, 'wb') as f:\n",
        "\n",
        "    pickle.dump(father_weights, f, pickle.HIGHEST_PROTOCOL)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMmOX-VSKTjQ"
      },
      "source": [
        "# Archiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aCkdHuhKUqV"
      },
      "outputs": [],
      "source": [
        "def accuracy_test(test_images,test_lbls):\n",
        "  acc_test_images=test_images.reshape(-1,28,28)\n",
        "  acc_test_images=acc_test_images[:, :,:,np.newaxis]\n",
        "  #print(acc_test_images.shape)\n",
        "\n",
        "  acc_test_images = conv_apply(weights, acc_test_images)\n",
        "  #print(imgs.shape)\n",
        "  acc_test_images = acc_test_images.reshape(*acc_test_images.shape[:1],-1) # Flatten\n",
        "\n",
        "\n",
        "  #print(\"test_lbls\",test_lbls.shape)\n",
        "  #print(\"test_images\",acc_test_images.shape)\n",
        "  return accuracy(MLP_params, acc_test_images, test_lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hiOWYv7Auwx"
      },
      "outputs": [],
      "source": [
        "#funkioniert nicht\n",
        "n_offspr=100\n",
        "fath_weights=father_weights\n",
        "\n",
        "statedic_list = jnp.empty(len(father_weights))\n",
        "#statedic_list=[]\n",
        "for i in range(0,n_offspr):\n",
        "  dicta = jnp.empty(0) \n",
        "  #dicta = [()] * len(father_weights)\n",
        "  for idx,w in enumerate(father_weights):\n",
        "      if w:\n",
        "          w, b = w\n",
        "          #print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))\n",
        "    \n",
        "          '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''\n",
        "          if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):\n",
        "            x_w=w\n",
        "            x_b=b\n",
        "          else:\n",
        "            #seed=random.randint(rng,(0,100000)\n",
        "            key = random.PRNGKey(0)\n",
        "            x_w = w+random.normal(key,shape=w.shape)*std_modifier\n",
        "            x_b = b+random.normal(key,shape=b.shape)*std_modifier\n",
        "            weight=jnp.empty(0) \n",
        "            weight=jnp.append(x_w,x_b, axis=None)\n",
        "          dicta=jnp.append(dicta,weight, axis=None)\n",
        "          print(jnp.shape(dicta))\n",
        "  statedic_list=jnp.append(statedic_list, dicta, axis=None)\n",
        "affe=statedic_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIjlPz3EOwkt"
      },
      "outputs": [],
      "source": [
        "conv_weights=jax.tree_map(lambda x: x+jax.random.normal(convu_key)*std_modifier, father_weights)\n",
        "\n",
        "diff=jax.tree_map(lambda x,y: x-y, father_weights,conv_weights)\n",
        "\n",
        "jax.flatten_util.ravel_pytree(conv_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMp15GHlUn9H"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "'''VMap/Batch of whole metaepoch. Runnning with 5 metaepochs, Kernel crushing with 10'''\n",
        "#vmap_offspring_run\n",
        "\n",
        "\n",
        "\n",
        "father_weights = conv_init(rng, (batch_size,28,28,1))\n",
        "father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple\n",
        "\n",
        "\n",
        "x_train=train_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]\n",
        "y_train=train_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]\n",
        "x_train=x_train.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)\n",
        "y_train=y_train.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size)\n",
        "\n",
        "test_img_off=test_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch\n",
        "test_lbl_off=test_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]\n",
        "test_img_off=test_img_off.reshape(n_metaepochs,n_offsp_epoch,n_test,28,28,1)\n",
        "test_lbl_off=test_lbl_off.reshape(n_metaepochs,n_offsp_epoch,n_test)\n",
        "\n",
        "print(jnp.shape(x_train))\n",
        "print(jnp.shape(y_train))\n",
        "print(jnp.shape(test_img_off))\n",
        "print(jnp.shape(test_lbl_off))\n",
        "key_matrix_seed=jnp.full((n_metaepochs,),np.arange(0, n_metaepochs, 1))\n",
        "\n",
        "affe=vmap_offspring_run(key_matrix_seed,x_train,y_train,test_img_off,test_lbl_off,rngkey_MLP)\n",
        "print(affe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lEycMlxVPLT"
      },
      "outputs": [],
      "source": [
        "'''VMap/Batch of whole metaepoch. Runnning with 5 metaepochs, Kernel crushing with 10'''\n",
        "def vmap_offspring_run(key_matrix_seed,x_train,y_train,test_img_off,test_lbl_off,rngkey_MLP):\n",
        "  return vmap(offspring_run, ( 0,0, 0,0,0,None))(key_matrix_seed,x_train,y_train,test_img_off,test_lbl_off,rngkey_MLP)\n",
        "\n",
        "@jit\n",
        "def offspring_run(key_matrix_seed,x_train,y_train,test_img_off,test_lbl_off,rngkey_MLP):\n",
        "\n",
        "  \n",
        "  \n",
        "  rng=jax.random.PRNGKey(key_matrix_seed)\n",
        "  key_matrix = conv_init(rng, (batch_size,28,28,1))[1]\n",
        "  key_matrix=jax.tree_map(lambda x: x*std_modifier, key_matrix)\n",
        "  conv_weights=jax.tree_map(lambda x,y: x+y, father_weights,key_matrix)\n",
        "\n",
        "  \n",
        "  result_off=jit_vmap_bootstrapp_offspring_MLP(rngkey_MLP,conv_weights,x_train,y_train,test_img_off,test_lbl_off)\n",
        "  return(jnp.mean(result_off))\n",
        "  #jnp.std(result_off)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQ1E2sRWfdp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5G_BSqVWvBw"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                                            transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                                            batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                                          transform=torchvision.transforms.Compose([\n",
        "                                          torchvision.transforms.ToTensor(),\n",
        "                                          torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                                          batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r86RJ_oFYFnc"
      },
      "outputs": [],
      "source": [
        "def save_p(obj, name,direc):\n",
        "    path=os.path.join(direc,name)\n",
        "    with open(path+\".pkl\", 'wb') as f:\n",
        "\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "def pathandstuff():\n",
        "\n",
        "    global save_txt\n",
        "    global base_path\n",
        "    global save_path\n",
        "\n",
        "    if os.path.exists(local_path):\n",
        "        '''Save running code file to log folder'''\n",
        "        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook\n",
        "        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder\n",
        "        print(\"on local\")\n",
        "        base_path=local_path\n",
        "    elif os.path.exists(googledrive_path):\n",
        "        print(\"on google\")\n",
        "        base_path=googledrive_path\n",
        "    else:\n",
        "        raise ValueError('Please specify save path or connect to Google Drive')\n",
        "        \n",
        "    logs_path=base_path+\"Logs/\"\n",
        "    '''Set logging and temp paths'''\n",
        "    timestamp=time.strftime(\"%d.%m.%Y_%H.%M\")\n",
        "    foldername=timestamp\n",
        "    save_path=os.path.join(logs_path,foldername)\n",
        "    print(\"Log path:\",save_path)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    save_txt = os.path.join(save_path, 'Log_MNist_{}.txt'.format(foldername))\n",
        "    \n",
        "   \n",
        "def plot_mean(mean_results,std_results): \n",
        "    mean=mean_results\n",
        "    std=std_results\n",
        "    x_axis=np.arange(0,len(mean), 1)\n",
        "\n",
        "    plt.xticks(range(0,len(mean)))\n",
        "\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "\n",
        "    lower_bound = mean - std\n",
        "    upper_bound = mean + std\n",
        "\n",
        "    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3)\n",
        "    plt.plot(mean,marker=\"o\")\n",
        "    plt.savefig(save_path+'/plot.png')\n",
        "    return plt    \n",
        "\n",
        "def sigma_decay(start, end, n_iter):\n",
        "  return(end/start)**(1/n_iter)\n",
        "\n",
        "def index_nhighest_list(n,list):\n",
        "  return [i for x, i in heapq.nlargest(n,((x, i) for i, x in enumerate(list)))]\n",
        "\n",
        "def matrix_distance(offspring_list):\n",
        "    length=len(offspring_list)\n",
        "\n",
        "    matrix=np.zeros((length, length))\n",
        "    for c in range(length):\n",
        "        for r in range(length):\n",
        "\n",
        "\n",
        "\n",
        "            hilf=substract_two_dics(offspring_list[c],offspring_list[r])\n",
        "\n",
        "            hilf2=square_dic(dict(hilf))\n",
        "            sum_=dic_totalsum(hilf2)\n",
        "            matrix[r][c]=sum_\n",
        "    return matrix\n",
        "    \n",
        "def substract_two_dics(a,b):\n",
        "    a = Counter(a)\n",
        "    a.subtract(b)\n",
        "    return a\n",
        "\n",
        "'''logging to txt and print'''\n",
        "def logg_to_file (string_, array=None):\n",
        "  if array is None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    \n",
        "  if array is not None:\n",
        "\n",
        "    file1 = open(save_txt,\"a\")\n",
        "    file1.write(string_)\n",
        "    file1.write(str(array))\n",
        "    file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "    \n",
        "def log_variables():\n",
        "    logg_to_file((\"seed = {}\".format(seed)))\n",
        "    logg_to_file ((\"n_training_epochs = {}\".format(n_training_epochs)))\n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    logg_to_file ((\"n_testing_epochs = {}\".format(n_testing_epochs)))\n",
        "    logg_to_file ((\"n_samples = {}\".format(n_samples)))\n",
        "    logg_to_file ((\"n_test = {}\".format(n_test)))\n",
        "    logg_to_file ((\"batch_size_train = {}\".format(batch_size_train)))\n",
        "    logg_to_file ((\"learning_rate = {}\".format(learning_rate)))\n",
        "\n",
        "    logg_to_file ((\"std_modifier = {}\".format(std_modifier)))\n",
        "    logg_to_file ((\"use_sigma_decay = {}\".format(use_sigma_decay)))\n",
        "    logg_to_file ((\"sigma_start = {}\".format(sigma_start)))\n",
        "    logg_to_file ((\"sigma_goal = {}\".format(sigma_goal)))\n",
        "    logg_to_file ((\"elitist = {}\".format(elitist)))\n",
        "    logg_to_file ((\"NNin1 = {}\".format(NNin1)))\n",
        "    logg_to_file ((\"NNout1 = {}\".format(NNout1)))\n",
        "    logg_to_file ((\"Convu_in1 = {}\".format(Convu_in1)))\n",
        "    logg_to_file ((\"Convu_out1 = {}\".format(Convu_out1)))\n",
        "    logg_to_file ((\"Convu_out2 = {}\".format(Convu_out2)))\n",
        "\n",
        "    logg_to_file ((\"kernelsize_ = {}\".format(kernelsize_)))\n",
        "    logg_to_file ((\"use_elitist = {}\".format(use_elitist)))\n",
        "    logg_to_file ((\"n_metaepochs = {}\".format(n_metaepochs)))\n",
        "    logg_to_file ((\"n_testing_epochs = {}\".format(n_testing_epochs)))\n",
        "                  \n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    logg_to_file ((\"n_offsprings = {}\".format(n_offsprings)))\n",
        "\n",
        "    logg_to_file ((\"use_softmax = {}\".format(use_softmax)))\n",
        "    logg_to_file ((\"temperature = {}\".format(temperature)))\n",
        "\n",
        "                  \n",
        "def square_dic(my_dict):\n",
        "    my_dict.update((x, y**2) for x, y in my_dict.items())\n",
        "    return my_dict   \n",
        "\n",
        "def dic_totalsum(my_dict):\n",
        "    return float(torch.sum(sum(sum(x) for x in my_dict.values())))\n",
        "\n",
        "'''weighting every element in dic list with softmax weights from m'''\n",
        "def weight_offspring_list(n_offsprings,offspring_list,m):\n",
        "    for n in range(n_offsprings):\n",
        "\n",
        "        for key in offspring_list[n]:    \n",
        "            offspring_list[n][key] *=  m[n]\n",
        "    return offspring_list\n",
        "\n",
        "'''needed for softmax'''\n",
        "def sum_list_of_dics(dic_list):\n",
        "    c = Counter()\n",
        "    for d in w_offspring_list:\n",
        "        c.update(d)\n",
        "    return c\n",
        "\n",
        "'''get best performer from final resultlist[[0,2,0.8],[1,2,0.5],[2,2,0.2]'''\n",
        "def best_index_bestperformer(lista):\n",
        "    hilf=0\n",
        "    index=None\n",
        "    for i in range (len(lista)):\n",
        "        if hilf<lista[i][2].item():\n",
        "            index=i\n",
        "            hilf=lista[i][2].item()\n",
        "    return index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-kL8Bx16naJ"
      },
      "outputs": [],
      "source": [
        "'''softmax from keylist for approach 1'''\n",
        "def softmax_conv_weights(father_weights,keylist, acc_list,temp):\n",
        "  temp_list=softmax(acc_list,temp)\n",
        "  for k in range(len(keylist)):\n",
        "        if k==0:\n",
        "          top_dog = father_weights\n",
        "          top_dog=jax.tree_map(lambda x: x*temp_list[k], top_dog)\n",
        "        else:\n",
        "          rng_soft = jax.random.PRNGKey(k)\n",
        "          general_dog = conv_init(rng_soft, (batch_size,28,28,1))[1]\n",
        "          top_dog=jax.tree_map(lambda x,y: x+y*std_modifier*temp_list[k], top_dog,general_dog)\n",
        "  return top_dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlV_GvOLbJxV"
      },
      "outputs": [],
      "source": [
        "'''#test better performer 1\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,(3,3), strides=(1,1),padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, (3,3), strides=(1,1),padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    \n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EcL5pQhg2Vy"
      },
      "outputs": [],
      "source": [
        "'''Convu1_in=12\n",
        "Convu2_in=24\n",
        "#test better performer 2, good performance\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(Convu1_in,(3,3), strides=(1,1),padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "    stax.Conv(Convu2_in, (3,3), strides=(1,1),padding=\"SAME\"),\n",
        "    stax.BatchNorm(),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool((2,2)),\n",
        "   \n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wRCJRfih-A3"
      },
      "outputs": [],
      "source": [
        "''' #Source Robert Lange\n",
        "conv_init, conv_apply = stax.serial(stax.Conv(12, (5, 5), (1, 1), padding=\"SAME\"),\n",
        "                                 stax.BatchNorm(), stax.Relu,\n",
        "                                 stax.Conv(24, (5, 5), (1, 1), padding=\"SAME\"),\n",
        "                                 stax.BatchNorm(), stax.Relu,\n",
        "                                 stax.Conv(24, (3, 3), (1, 1), padding=\"SAME\"),\n",
        "                                 stax.BatchNorm(), stax.Relu,\n",
        "                                 stax.Conv(10, (3, 3), (1, 1), padding=\"SAME\"), stax.BatchNorm(),stax.Relu\n",
        "                                 )'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB6yy5oI-HHi"
      },
      "outputs": [],
      "source": [
        "'''train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([\n",
        "                                            torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "train_images = jnp.array(train_dataset.data,dtype=\"float32\").reshape(len(train_dataset), -1)\n",
        "train_lbls = jnp.array(train_dataset.targets)\n",
        "\n",
        "test_images = jnp.array(test_dataset.data,dtype=\"float32\").reshape(len(test_dataset), -1)\n",
        "test_lbls = jnp.array(test_dataset.targets)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frl6fjmWAcUF"
      },
      "outputs": [],
      "source": [
        "file = open(\"/content/long_run_fatherweights_04.04.22.pkl\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "with open(r\"/content/long_run_fatherweights_04.04.22.pkl\", \"rb\") as input_file:\n",
        "  father_weights = cPickle.load(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmOUKRGMsPLw"
      },
      "outputs": [],
      "source": [
        "def log_variables():\n",
        "    \n",
        "    logg_to_file ((\"n_training_epochs = {}\".format(n_training_epochs)))\n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    \n",
        "    logg_to_file ((\"n_samples = {}\".format(n_samples)))\n",
        "    logg_to_file ((\"n_test = {}\".format(n_test)))\n",
        "    logg_to_file ((\"batch_size = {}\".format(batch_size)))\n",
        "    \n",
        "\n",
        "    logg_to_file ((\"std_modifier = {}\".format(std_modifier)))\n",
        "    logg_to_file ((\"use_sigma_decay = {}\".format(use_sigma_decay)))\n",
        "    logg_to_file ((\"sigma_start = {}\".format(sigma_start)))\n",
        "    logg_to_file ((\"sigma_goal = {}\".format(sigma_goal)))\n",
        "    logg_to_file ((\"n_decay_epochs = {}\".format(n_decay_epochs)))\n",
        "    logg_to_file ((\"use_pickle = {}\".format(use_pickle)))\n",
        "    logg_to_file ((\"use_father = {}\".format(use_father)))\n",
        "\n",
        "    logg_to_file ((\"NNin1 = {}\".format(NNin1)))\n",
        "    logg_to_file ((\"NNout1 = {}\".format(NNout1)))\n",
        "    logg_to_file ((\"Convu_in1 = {}\".format(Convu_in1)))\n",
        "    logg_to_file ((\"Convu_out1 = {}\".format(Convu_out1)))\n",
        "    logg_to_file ((\"Convu_out2 = {}\".format(Convu_out2)))\n",
        "\n",
        "    logg_to_file ((\"kernelsize_ = {}\".format(kernelsize_)))\n",
        "    \n",
        "    logg_to_file ((\"n_metaepochs = {}\".format(n_metaepochs)))\n",
        "    logg_to_file ((\"n_testing_epochs = {}\".format(n_testing_epochs)))\n",
        "                  \n",
        "    logg_to_file ((\"n_offsp_epoch = {}\".format(n_offsp_epoch)))\n",
        "    logg_to_file ((\"n_offsprings = {}\".format(n_offsprings)))\n",
        "\n",
        "    logg_to_file ((\"use_softmax = {}\".format(use_softmax)))\n",
        "    logg_to_file ((\"temperature = {}\".format(temperature)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "JAX_MNist",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}