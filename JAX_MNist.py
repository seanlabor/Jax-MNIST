# -*- coding: utf-8 -*-
"""JAX_MNist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V4_9kdTf8sZ152W0MAgVNBpEpVsjMTVg

# MLP training on MNIST

1.   Testing on all 10000 images,because faster. Problem?
2.   Listeneintrag
"""

import numpy as np
import jax.numpy as jnp
from jax.scipy.special import logsumexp
import jax
from jax import jit, vmap, pmap, grad, value_and_grad
import random
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from jax.example_libraries import stax, optimizers
import torchvision
import torch
import torch.utils.data as data_utils

import time
from jax.example_libraries import stax
from jax.example_libraries.stax import Dense, Relu, LogSoftmax

from jax import random

"""# **Funktions**"""

'''Set your file directorys'''

googledrive_path="/content/drive/MyDrive/Colab Notebooks/MNist/"
local_path="C:/Users/Flo/Documents/Uni/Masterarbeit/Hanabi/Mnist handwritten digits/"

'''set global seeds'''



'''number of training epochs for Network2'''
n_training_epochs = 3 #number of training epochs for every NN2.  
n_offsp_epoch = 10 #number of training and testing runs combined, to get an average for the performance of the Convu net.
n_testing_epochs = 3 # number of testing runs, per n_offsp_epoch

'''parameter Network2'''
n_samples = 150  #Number of training samples for NN2, distribution of data for 600: [68, 59, 66, 67, 47, 46, 65, 71, 53, 58])
n_test=1000 #Number of test samples for NN2. Needs to be multiples of batch_size_test=500 )
batch_size = 50 # for precise n_samples number must be: n_samples%batch_size_train=0

learning_rate = 0.1
momentum = 0.5
log_interval = 10



'''Standard deviation for gaussian noise in Network1'''
'''!!! Important hyperparameter, >=1 gives bad results'''
std_modifier=0.05


# !!! ATM Overriden in main code part
'''number of offsprings per metaepoch'''
n_offsprings=100
'''number of metaopochs'''
n_metaepochs=10


NNin1=2500 #dependent on Convu
NNout1=10

'''Convunet'''
Convu1_in=32
Convu2_in=16
Convu3_in=4
seed_convu=0

n_samples=150 #number of training samples
batch_size = 50
n_test=1000
n_training_epochs=3
print_distribution_data=False
std_modifier=0.05


Convu1_in=32
Convu2_in=16
Convu3_in=4



use_sigma_decay=True
sigma_start=1.0
sigma_goal=0.1

'''logging to screen variables'''
print_offsprings=True
print_distribution_data=False
use_sigma_decay=True #otherwise using constant sigma from config tab
sigma_start=1 
sigma_goal=0.05 #sigma goal after n_metaepochs
        
'''choose either method, softmax or elitist=keep only best offspring'''
use_softmax=True
temperature=0.05
use_elitist=False
use_winnerlist=False

n_metaepochs=30 #overwriting variable from config tab, delete later
n_offsprings=10 #overwriting variable from config tab, delete later

'''number of training epochs for Network2'''
n_offsp_epoch = 2
n_testing_epochs = 5
n_metaepochs=10

train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))

test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))

train_images = jnp.array(train_dataset.data,dtype="float32").reshape(len(train_dataset), -1)
train_lbls = jnp.array(train_dataset.targets)

test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)



def init_MLP(layer_widths, parent_key, scale=0.01):

    params = []
    keys = jax.random.split(parent_key, num=len(layer_widths)-1)

    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):
        weight_key, bias_key = jax.random.split(key)
        params.append([
                       scale*jax.random.normal(weight_key, shape=(out_width, in_width)),
                       scale*jax.random.normal(bias_key, shape=(out_width,))
                       ]
        )
    return params

# test
#key = jax.random.PRNGKey(seed)
#MLP_params = init_MLP([2500, 10], key)
#print(jax.tree_map(lambda x: x.shape, MLP_params))

conv_init, conv_apply = stax.serial(
    stax.Conv(Convu1_in,(3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu2_in, (3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu3_in, (3,3), padding="SAME"),
    stax.Relu,
    stax.MaxPool((2,2))
)

@jit
def MLP_predict(params, x):

    hidden_layers = params[:-1]
    activation = x

    for w, b in hidden_layers:
        activation = jax.nn.relu(jnp.dot(w, activation) + b)

    w_last, b_last = params[-1]
    logits = jnp.dot(w_last, activation) + b_last

    return logits - logsumexp(logits)

jit_MLP_predict=jit(MLP_predict)

@jit
def batched_MLP_predict(params,x):
  return vmap(jit_MLP_predict, ( None, 0))(params,x)
  
jit_batched_MLP_predict=jit(batched_MLP_predict)

@jit
def loss_fn(params, imgs, gt_lbls):
  
    predictions = jit_batched_MLP_predict(params, imgs)
    #print("predictions",predictions.shape)
    return -jnp.mean(predictions * gt_lbls)


@jit
def update(params, imgs, gt_lbls, lr=0.01):
    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)

    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)

jit_update=jit(update)

@jit
def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):

    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,28,28,1))
    pred_classes = jnp.argmax(jit_batched_MLP_predict(MLP_params, imgs.reshape(-1,2500)), axis=1)

    return jnp.mean(dataset_lbls == pred_classes)
    
jit_accuracy=jit(accuracy)

'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''
@jit
def train(conv_weights, imgs, lbls,MLP_params ):
  
  for i in range(jnp.shape(imgs)[0]):

    gt_labels = jax.nn.one_hot(lbls[i], 10)
    img_conv = conv_apply(conv_weights, imgs[i].reshape(-1,28,28,1))
    loss, MLP_params = jit_update(MLP_params, img_conv.reshape(-1,2500), gt_labels)

  return MLP_params
  
jit_train=jit(train)

'''
Running for every offspring n_offsp_epoch loops to get stable acc results. 
Every loop is trained with n_samples/batch_size * batch size training epochs.
Everything put in jit and vmap to speed up

Input  
(10, 6, 25, 28, 28, 1) train_img_off
(10, 6, 25) train_lbl_off
(10, 1000, 28, 28, 1) test_img_off
(10, 1000) test_lbl_off
          
(n_offsp_epoch, n_samples/batch_size, batch size, 28, 28, 1)
(n_offsp_epoch, n_samples/batch_size, batch size)
(n_offsp_epoch, n_test, 28, 28, 1)
(n_offsp_epoch, n_test)'''

@jit
def bootstrapp_offspring_MLP(key,conv_weights, batch_affe, labelaffe,test_images,test_lbls):
  
  
  MLP_params = init_MLP([NNin1, NNout1], key)
  MLP_params_trained=jit_train(conv_weights, batch_affe, labelaffe,MLP_params )
 
  
  result=jit_accuracy(father_weights,MLP_params_trained,test_images,test_lbls)
  return (result)

jit_bootstrapp_offspring_MLP=jit(bootstrapp_offspring_MLP)  

@jit
def vmap_bootstrapp_offspring_MLP(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):
  return vmap(jit_bootstrapp_offspring_MLP, ( None,None, 0,0,0,0))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)
  
jit_vmap_bootstrapp_offspring_MLP=jit(vmap_bootstrapp_offspring_MLP)

'''Initialize Variables'''

n_samples = 150
batch_size = 25
n_offsp_epoch=5
n_metaepochs=5

n_offsprings=100
starting_key=59 #define starting point
create_offsprings_bool=True
std_modifier=0.05
temp=0.01 #weight for softmax

# Commented out IPython magic to ensure Python compatibility.
# #main code
# %%time
# 
# for meta in range (n_metaepochs):
#     
#     if meta ==0:
#         father_key=jax.random.PRNGKey(starting_key)
#         father_weights = conv_init(father_key, (batch_size,28,28,1))
#         father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple
#         offspring_list=create_offsprings(n_offsprings, father_weights)
#     if meta >=1:
#         if create_offsprings_bool:
#           father_weights=softmax_offlist(offspring_list,[x[0] for x in result_list_metaepoch],temp)
#           offspring_list=create_offsprings(n_offsprings, father_weights)
#         else:
#           father_weights=softmax_conv_weights(father_weights,keylist_metaepoch, [x[0] for x in result_list_metaepoch],0.02)
#             
#     result_list_metaepoch=[]
#     keylist_metaepoch=[]
#     for i in range(n_offsprings):
#       if create_offsprings_bool:
#         conv_weights=offspring_list[i]
#       else:
#         keylist_metaepoch.append(starting_key+meta+i)
#         rng = jax.random.PRNGKey(starting_key+meta+i)
# 
#         key_matrix = conv_init(rng, (batch_size,28,28,1))[1]
#         key_matrix=jax.tree_map(lambda x: x*std_modifier, key_matrix)
#         conv_weights=jax.tree_map(lambda x,y: x+y, father_weights,key_matrix)
# 
# 
# 
#       train_img_off=train_images[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
#       train_lbl_off=train_lbls[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
#       train_img_off=train_img_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)
#       train_lbl_off=train_lbl_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size)
# 
#       test_img_off=test_images[random.randint(rng, (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
#       test_lbl_off=test_lbls[random.randint(rng, (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]
#       test_img_off=test_img_off.reshape(n_offsp_epoch,n_test,28,28,1)
#       test_lbl_off=test_lbl_off.reshape(n_offsp_epoch,n_test)
# 
#       #print(jnp.shape(train_img_off))
#       #print(jnp.shape(train_lbl_off))
#       #print(jnp.shape(test_img_off))
#       #print(jnp.shape(test_lbl_off))
# 
# 
#       result_off=jit_vmap_bootstrapp_offspring_MLP(rng,conv_weights,train_img_off,train_lbl_off,test_img_off,test_lbl_off)
#       #result=result.append([jnp.mean(result_off),jnp.std(result_off)])
#       result_list_metaepoch.append([float(jnp.mean(result_off)),float(jnp.std(result_off))])
#     print(np.mean(np.array(result_list_metaepoch), axis=0))



'''working but going different approach with pytree.mapping'''
def create_offsprings(n_offspr, fath_weights):
  statedic_list=[]
  for i in range(0,n_offspr):
    dicta = [()] * len(father_weights)
    for idx,w in enumerate(father_weights):
        if w:
            w, b = w
            #print("Weights : {}, Biases : {}".format(w.shape, b.shape))
      
            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''
            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):
              x_w=w
              x_b=b
            else:
              #seed=random.randint(rng,(0,100000)
              key = random.PRNGKey(0)
              x_w = w+random.normal(key,shape=w.shape)*std_modifier
              x_b = b+random.normal(key,shape=b.shape)*std_modifier
            dicta[idx]=(x_w,x_b)
    
    statedic_list.append(dicta)
  return statedic_list

def softmax_offlist(off_list,acc_list,temp):
  temp_list=softmax_result(acc_list,temp)
  for i in range(len(off_list)):
    if i==0:
      top_dog=jax.tree_map(lambda x: x*temp_list[i], off_list[i])
    else:
      general_dog = jax.tree_map(lambda x: x*temp_list[i], off_list[i])
      top_dog=jax.tree_map(lambda x,y: x+y, top_dog,general_dog)
  return top_dog

''''''
def softmax_conv_weights(father_weights,keylist, acc_list,temp):
  temp_list=softmax(acc_list,temp)
  for k in range(len(keylist)):
        if k==0:
          top_dog = father_weights
          top_dog=jax.tree_map(lambda x: x*temp_list[k], top_dog)
        else:
          rng_soft = jax.random.PRNGKey(k)
          general_dog = conv_init(rng_soft, (batch_size,28,28,1))[1]
          top_dog=jax.tree_map(lambda x,y: x+y*std_modifier*temp_list[k], top_dog,general_dog)
  return top_dog

'''Creates softmax/temp list out of accuracy list [0.2,0.3,....,0.8]'''
def softmax_result(results,temp: float):
    x = [z/temp for z in results]
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

"""# **Testing**"""

hund_key=jax.random.PRNGKey(121)
hund = conv_init(hund_key, (batch_size,28,28,1))
hund = hund[1] ## Weights are actually stored in second element of two value tuple

affe_key=jax.random.PRNGKey(122)
affe = conv_init(affe_key, (batch_size,28,28,1))
affe = affe[1] ## Weights are actually stored in second element of two value tuple

temp1=0.2
temp2=0.4
diff=jax.tree_map(lambda x,y: (x*temp1+y*temp2), hund,affe)



top_dog

"""# Archiv"""

def accuracy_test(test_images,test_lbls):
  acc_test_images=test_images.reshape(-1,28,28)
  acc_test_images=acc_test_images[:, :,:,np.newaxis]
  #print(acc_test_images.shape)

  acc_test_images = conv_apply(weights, acc_test_images)
  #print(imgs.shape)
  acc_test_images = acc_test_images.reshape(*acc_test_images.shape[:1],-1) # Flatten


  #print("test_lbls",test_lbls.shape)
  #print("test_images",acc_test_images.shape)
  return accuracy(MLP_params, acc_test_images, test_lbls)

#funkioniert nicht
n_offspr=100
fath_weights=father_weights

statedic_list = jnp.empty(len(father_weights))
#statedic_list=[]
for i in range(0,n_offspr):
  dicta = jnp.empty(0) 
  #dicta = [()] * len(father_weights)
  for idx,w in enumerate(father_weights):
      if w:
          w, b = w
          #print("Weights : {}, Biases : {}".format(w.shape, b.shape))
    
          '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''
          if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):
            x_w=w
            x_b=b
          else:
            #seed=random.randint(rng,(0,100000)
            key = random.PRNGKey(0)
            x_w = w+random.normal(key,shape=w.shape)*std_modifier
            x_b = b+random.normal(key,shape=b.shape)*std_modifier
            weight=jnp.empty(0) 
            weight=jnp.append(x_w,x_b, axis=None)
          dicta=jnp.append(dicta,weight, axis=None)
          print(jnp.shape(dicta))
  statedic_list=jnp.append(statedic_list, dicta, axis=None)
affe=statedic_list

conv_weights=jax.tree_map(lambda x: x+jax.random.normal(convu_key)*std_modifier, father_weights)

diff=jax.tree_map(lambda x,y: x-y, father_weights,conv_weights)

jax.flatten_util.ravel_pytree(conv_weights)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# '''VMap/Batch of whole metaepoch. Runnning with 5 metaepochs, Kernel crushing with 10'''
# #vmap_offspring_run
# 
# 
# 
# father_weights = conv_init(rng, (batch_size,28,28,1))
# father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple
# 
# 
# train_img_off=train_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]
# train_lbl_off=train_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]
# train_img_off=train_img_off.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)
# train_lbl_off=train_lbl_off.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size)
# 
# test_img_off=test_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
# test_lbl_off=test_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]
# test_img_off=test_img_off.reshape(n_metaepochs,n_offsp_epoch,n_test,28,28,1)
# test_lbl_off=test_lbl_off.reshape(n_metaepochs,n_offsp_epoch,n_test)
# 
# print(jnp.shape(train_img_off))
# print(jnp.shape(train_lbl_off))
# print(jnp.shape(test_img_off))
# print(jnp.shape(test_lbl_off))
# key_matrix_seed=jnp.full((n_metaepochs,),np.arange(0, n_metaepochs, 1))
# 
# affe=vmap_offspring_run(key_matrix_seed,train_img_off,train_lbl_off,test_img_off,test_lbl_off,rngkey_MLP)
# print(affe)

'''VMap/Batch of whole metaepoch. Runnning with 5 metaepochs, Kernel crushing with 10'''
def vmap_offspring_run(key_matrix_seed,train_img_off,train_lbl_off,test_img_off,test_lbl_off,rngkey_MLP):
  return vmap(offspring_run, ( 0,0, 0,0,0,None))(key_matrix_seed,train_img_off,train_lbl_off,test_img_off,test_lbl_off,rngkey_MLP)

@jit
def offspring_run(key_matrix_seed,train_img_off,train_lbl_off,test_img_off,test_lbl_off,rngkey_MLP):

  
  
  rng=jax.random.PRNGKey(key_matrix_seed)
  key_matrix = conv_init(rng, (batch_size,28,28,1))[1]
  key_matrix=jax.tree_map(lambda x: x*std_modifier, key_matrix)
  conv_weights=jax.tree_map(lambda x,y: x+y, father_weights,key_matrix)

  
  result_off=jit_vmap_bootstrapp_offspring_MLP(rngkey_MLP,conv_weights,train_img_off,train_lbl_off,test_img_off,test_lbl_off)
  return(jnp.mean(result_off))
  #jnp.std(result_off)



train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,
                                            transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                            batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)

test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,
                                          transform=torchvision.transforms.Compose([
                                          torchvision.transforms.ToTensor(),
                                          torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                          batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn,drop_last=True)

def save_p(obj, name,direc):
    path=os.path.join(direc,name)
    with open(path+".pkl", 'wb') as f:

        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
        f.close()


    
'''logging to txt and print'''
def logg (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    print(string_)
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    print(string_, array)
  
def pathandstuff():

    global save_txt
    global base_path
    global save_path

    if os.path.exists(local_path):
        '''Save running code file to log folder'''
        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook
        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder
        print("on local")
        base_path=local_path
    elif os.path.exists(googledrive_path):
        print("on google")
        base_path=googledrive_path
    else:
        raise ValueError('Please specify save path or connect to Google Drive')
        
    logs_path=base_path+"Logs/"
    '''Set logging and temp paths'''
    timestamp=time.strftime("%d.%m.%Y_%H.%M")
    foldername=timestamp
    save_path=os.path.join(logs_path,foldername)
    print("Log path:",save_path)
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    save_txt = os.path.join(save_path, 'Log_MNist_{}.txt'.format(foldername))
    
   
def plot_mean(mean_results,std_results): 
    mean=mean_results
    std=std_results
    x_axis=np.arange(0,len(mean), 1)

    plt.xticks(range(0,len(mean)))

    mean = np.array(mean)
    std = np.array(std)

    lower_bound = mean - std
    upper_bound = mean + std

    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3)
    plt.plot(mean,marker="o")
    plt.savefig(save_path+'/plot.png')
    return plt    

def sigma_decay(start, end, n_iter):
  return(end/start)**(1/n_iter)

def index_nhighest_list(n,list):
  return [i for x, i in heapq.nlargest(n,((x, i) for i, x in enumerate(list)))]

def matrix_distance(offspring_list):
    length=len(offspring_list)

    matrix=np.zeros((length, length))
    for c in range(length):
        for r in range(length):



            hilf=substract_two_dics(offspring_list[c],offspring_list[r])

            hilf2=square_dic(dict(hilf))
            sum_=dic_totalsum(hilf2)
            matrix[r][c]=sum_
    return matrix
    
def substract_two_dics(a,b):
    a = Counter(a)
    a.subtract(b)
    return a

'''logging to txt and print'''
def logg_to_file (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    
def log_variables():
    logg_to_file(("seed = {}".format(seed)))
    logg_to_file (("n_training_epochs = {}".format(n_training_epochs)))
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
    logg_to_file (("n_samples = {}".format(n_samples)))
    logg_to_file (("n_test = {}".format(n_test)))
    logg_to_file (("batch_size_train = {}".format(batch_size_train)))
    logg_to_file (("learning_rate = {}".format(learning_rate)))

    logg_to_file (("std_modifier = {}".format(std_modifier)))
    logg_to_file (("use_sigma_decay = {}".format(use_sigma_decay)))
    logg_to_file (("sigma_start = {}".format(sigma_start)))
    logg_to_file (("sigma_goal = {}".format(sigma_goal)))
    logg_to_file (("elitist = {}".format(elitist)))
    logg_to_file (("NNin1 = {}".format(NNin1)))
    logg_to_file (("NNout1 = {}".format(NNout1)))
    logg_to_file (("Convu_in1 = {}".format(Convu_in1)))
    logg_to_file (("Convu_out1 = {}".format(Convu_out1)))
    logg_to_file (("Convu_out2 = {}".format(Convu_out2)))

    logg_to_file (("kernelsize_ = {}".format(kernelsize_)))
    logg_to_file (("use_elitist = {}".format(use_elitist)))
    logg_to_file (("n_metaepochs = {}".format(n_metaepochs)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
                  
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_offsprings = {}".format(n_offsprings)))

    logg_to_file (("use_softmax = {}".format(use_softmax)))
    logg_to_file (("temperature = {}".format(temperature)))

                  
def square_dic(my_dict):
    my_dict.update((x, y**2) for x, y in my_dict.items())
    return my_dict   

def dic_totalsum(my_dict):
    return float(torch.sum(sum(sum(x) for x in my_dict.values())))

'''weighting every element in dic list with softmax weights from m'''
def weight_offspring_list(n_offsprings,offspring_list,m):
    for n in range(n_offsprings):

        for key in offspring_list[n]:    
            offspring_list[n][key] *=  m[n]
    return offspring_list

'''needed for softmax'''
def sum_list_of_dics(dic_list):
    c = Counter()
    for d in w_offspring_list:
        c.update(d)
    return c

'''get best performer from final resultlist[[0,2,0.8],[1,2,0.5],[2,2,0.2]'''
def best_index_bestperformer(lista):
    hilf=0
    index=None
    for i in range (len(lista)):
        if hilf<lista[i][2].item():
            index=i
            hilf=lista[i][2].item()
    return index