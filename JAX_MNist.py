# -*- coding: utf-8 -*-
"""JAX_MNist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V4_9kdTf8sZ152W0MAgVNBpEpVsjMTVg

# MLP training on MNIST

1.   Testing on all 10000 images,because faster. Problem?
2.   Listeneintrag
"""

import numpy as np
import jax.numpy as jnp
from jax.scipy.special import logsumexp
import jax
from jax import jit, vmap, pmap, grad, value_and_grad
import random
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from jax.example_libraries import stax, optimizers
import torchvision
import torch
import torch.utils.data as data_utils
from jax import numpy as jnp
import time
import random

seed = 0
mnist_img_size = (28, 28)

def init_MLP(layer_widths, parent_key, scale=0.01):

    params = []
    keys = jax.random.split(parent_key, num=len(layer_widths)-1)

    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):
        weight_key, bias_key = jax.random.split(key)
        params.append([
                       scale*jax.random.normal(weight_key, shape=(out_width, in_width)),
                       scale*jax.random.normal(bias_key, shape=(out_width,))
                       ]
        )

    return params

# test
key = jax.random.PRNGKey(seed)
MLP_params = init_MLP([2500, 10], key)
print(jax.tree_map(lambda x: x.shape, MLP_params))

def MLP_predict(params, x):
    hidden_layers = params[:-1]

    activation = x
    for w, b in hidden_layers:
        activation = jax.nn.relu(jnp.dot(w, activation) + b)

    w_last, b_last = params[-1]
    logits = jnp.dot(w_last, activation) + b_last

    # log(exp(o1)) - log(sum(exp(o1), exp(o2), ..., exp(o10)))
    # log( exp(o1) / sum(...) )
    return logits - logsumexp(logits)

# tests

# test single example


# test batched function
batched_MLP_predict = vmap(MLP_predict, in_axes=(None, 0))

dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))
print(dummy_imgs_flat.shape)
#predictions = batched_MLP_predict(MLP_params, dummy_imgs_flat)
#print(predictions.shape)

conv_init, conv_apply = stax.serial(
    stax.Conv(Convu1_in,(3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu2_in, (3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu3_in, (3,3), padding="SAME"),
    stax.Relu,
    stax.MaxPool((2,2))
)

def create_offsprings(n_offspr, fath_weights):
  statedic_list=[]
  for i in range(0,100):
    dicta = [()] * len(father_weights)
    for idx,w in enumerate(father_weights):
        if w:
            w, b = w
            #print("Weights : {}, Biases : {}".format(w.shape, b.shape))
      
            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''
            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):
              x_w=w
              x_b=b
            else:
              #seed=random.randint(rng,(0,100000)
              key = random.PRNGKey(0)
              x_w = w+random.normal(key,shape=w.shape)*std_modifier
              x_b = b+random.normal(key,shape=b.shape)*std_modifier
            dicta[idx]=(x_w,x_b)
    
    statedic_list.append(dicta)
  return statedic_list

@jit
def loss_fn(params, imgs, gt_lbls):
  
    predictions = batched_MLP_predict(params, imgs)
    #print("predictions",predictions.shape)
    return -jnp.mean(predictions * gt_lbls)


@jit
def update(params, imgs, gt_lbls, lr=0.01):
    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)

    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)

jit_accuracy=jit(accuracy)
jit_update=jit(update)
jit_train=jit(train)

@jit
def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):

    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,28,28,1))
    pred_classes = jnp.argmax(batched_MLP_predict(MLP_params, imgs.reshape(-1,2500)), axis=1)

    return jnp.mean(dataset_lbls == pred_classes)

'''For loop is neccesary to do batch training. Every update iteration needs to run with updated MPL params'''
@jit
def train(conv_weights, imgs, lbls,MLP_params ):
  
  for i in range(jnp.shape(imgs)[0]):

    gt_labels = jax.nn.one_hot(lbls[i], 10)
    affe = conv_apply(conv_weights, imgs[i].reshape(-1,28,28,1))
    loss, MLP_params = jit_update(MLP_params, affe.reshape(-1,2500), gt_labels)

  return MLP_params

'''Input  
(10, 6, 25, 28, 28, 1) train_img_off
(10, 6, 25) train_lbl_off
(10, 1000, 28, 28, 1) test_img_off
(10, 1000) test_lbl_off
          
(n_offsp_epoch, n_samples/batch_size, batch size, 28, 28, 1)
(n_offsp_epoch, n_samples/batch_size, batch size)
(n_offsp_epoch, n_test, 28, 28, 1)
(n_offsp_epoch, n_test)'''
def bootstrapp_offspring_MLP(key,conv_weights, batch_affe, labelaffe,test_images,test_lbls):

  MLP_params = init_MLP([NNin1, NNout1], key)
  MLP_params_trained=jit_train(conv_weights, batch_affe, labelaffe,MLP_params )
 
  
  result=jit_accuracy(father_weights,MLP_params_trained,test_images,test_lbls)
  return (result)


def vmap_bootstrapp_offspring_MLP(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):
  return vmap(bootstrapp_offspring_MLP, ( None,None, 0,0,0,0))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)

#Test vmap_bootstrapp_offspring

import jax.numpy as np
from jax import vmap
from jax import random



rng = jax.random.PRNGKey(123)

father_weights = conv_init(rng, (batch_size,28,28,1))
father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple

affe=create_offsprings(100, father_weights)
father_weights=affe[0]

n_samples = 150
batch_size = 25
n_offsp_epoch=10

train_img_off=train_images[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
train_lbl_off=train_lbls[random.randint(rng, (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
train_img_off=train_img_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)
train_lbl_off=train_lbl_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size)

test_img_off=test_images[random.randint(rng, (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
test_lbl_off=test_lbls[random.randint(rng, (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]
test_img_off=test_img_off.reshape(n_offsp_epoch,n_test,28,28,1)
test_lbl_off=test_lbl_off.reshape(n_offsp_epoch,n_test)

print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print(jnp.shape(test_img_off))
print(jnp.shape(test_lbl_off))


result_off=vmap_bootstrapp_offspring_MLP(rng,father_weights,train_img_off,train_lbl_off,test_img_off,test_lbl_off)
print(jnp.mean(result_off))
print(jnp.std(result_off))



print(len(statedic_list[0][0][0]))
print(len(father_weights[0][0]))
print(type(statedic_list[0][0]))
print(type(father_weights[0]))

affe=create_offsprings(100, father_weights)

affe[0]

'''metaepoch data'''
train_images = jnp.array(train_dataset.data,dtype="float32").reshape(len(train_dataset), -1)
train_lbls = jnp.array(train_dataset.targets)

test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)


train_img_off=train_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]
train_lbl_off=train_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_samples), 0, 60000, dtype='uint8')]
print("test")
print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print("test")
train_img_off=train_img_off.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)
train_lbl_off=train_lbl_off.reshape(n_metaepochs,n_offsp_epoch,int((n_samples/batch_size)),batch_size)

test_img_off=test_images[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
test_lbl_off=test_lbls[random.randint(rng, (n_metaepochs,n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]
test_img_off=test_img_off.reshape(n_metaepochs,n_offsp_epoch,n_test,28,28,1)
test_lbl_off=test_lbl_off.reshape(n_metaepochs,n_offsp_epoch,n_test)

print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print(jnp.shape(test_img_off))
print(jnp.shape(test_lbl_off))

print(jnp.shape(test_img_off))

n_metaepochs







'''Initialize Variables'''
n_metaepochs=100
n_offsprings=100
n_offsp_epoch=1

rng = jax.random.PRNGKey(123)

father_weights = conv_init(rng, (batch_size,28,28,1))
father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple

# Commented out IPython magic to ensure Python compatibility.
# %%time
# mean_results=[]
# std_results=[]
# best_performer=[]
# result_list_final=[]
# 
# #pathandstuff()
# #log_variables()
# 
# 
# 
# for m in range (n_metaepochs):
#     #print("Meta:",m)
#    
# 
#     metaseed=0+m #change seed and therefore data input for every metaepoch
#     torch.manual_seed(seed)
#     start_meta = time.time()
#     #logg("Metaepoch:{}".format(m))
# 
#     if m ==0:
#         rng = jax.random.PRNGKey(seed_convu)
#         father_weights = conv_init(rng, (batch_size,28,28,1))[1]
#         #offspring_list=create_offsprings(n_offsprings, father_weights)
# 
#     if m >1000:
#         if use_softmax==True:
#             results=np.array([float(x[2]) for x in result_list_final[m-1]])
#             softmax_vec = softmax(results, temperature)
#             print("Softmax_vec:", softmax_vec)
#             w_offspring_list=weight_offspring_list(n_offsprings,offspring_list,softmax_vec)
#             winner=dict(deepcopy(sum_list_of_dics(w_offspring_list)))
#             offspring_list=create_offsprings(n_offsprings, winner)
#             
#         
# 
#     result_list_metaepoch=[]
#     best_accur=0
#     #create_log()
#     #logg("Metaepoch:{}".format(n_metaepochs))
# 
#      
#     for offsp_count in range (0, n_offsprings):
#        
#         offsp_resulst_list=[]
#         for offsp_epoch in range (n_offsp_epoch):
# 
# 
#             #convu_weig=offspring_list[offsp_count]
#             key = jax.random.PRNGKey(seed_convu)
#             
#             loss, acc=jit_train(key,father_weights)
#             offsp_resulst_list.append([offsp_count,loss, acc])
#             #print([loss, acc])
# 
#         result_list_metaepoch.append(offsp_resulst_list)
# 
# 
# print(offsp_epoch_list)

"""# **Testing**"""

test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)

from jax import random

random.randint(random.PRNGKey(0), (10000,), 0, 10000, dtype='uint8')

np.random.randint(10000, size=150)
a=(test_lbls)[np.random.randint(10000, size=150)]
a[0]

## Accuracy testing
test_img_off=test_images[random.randint(random.PRNGKey(0), (n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
test_lbl_off=test_lbls[random.randint(random.PRNGKey(0), (n_test,), 0, 10000, dtype='uint8')]
test_img_off=test_img_off.reshape(n_test,28,28,1)
test_lbl_off=test_lbl_off.reshape(n_test)
print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print(jnp.shape(test_img_off))
print(jnp.shape(test_lbl_off))


imgs = conv_apply(father_weights, test_img_off.reshape(-1,28,28,1))
print(jnp.shape(imgs))
pred_classes = jnp.argmax(batched_MLP_predict(MLP_params, imgs.reshape(-1,2500)), axis=1)

print(jnp.mean(test_lbl_off == pred_classes))

"""# **Funktions**"""

'''Set your file directorys'''

googledrive_path="/content/drive/MyDrive/Colab Notebooks/MNist/"
local_path="C:/Users/Flo/Documents/Uni/Masterarbeit/Hanabi/Mnist handwritten digits/"

'''set global seeds'''





'''number of training epochs for Network2'''
n_training_epochs = 3 #number of training epochs for every NN2.  
n_offsp_epoch = 10 #number of training and testing runs combined, to get an average for the performance of the Convu net.
n_testing_epochs = 3 # number of testing runs, per n_offsp_epoch

'''parameter Network2'''
n_samples = 150  #Number of training samples for NN2, distribution of data for 600: [68, 59, 66, 67, 47, 46, 65, 71, 53, 58])
n_test=1000 #Number of test samples for NN2. Needs to be multiples of batch_size_test=500 )
batch_size = 50 # for precise n_samples number must be: n_samples%batch_size_train=0

learning_rate = 0.1
momentum = 0.5
log_interval = 10



'''Standard deviation for gaussian noise in Network1'''
'''!!! Important hyperparameter, >=1 gives bad results'''
std_modifier=0.05


# !!! ATM Overriden in main code part
'''number of offsprings per metaepoch'''
n_offsprings=100
'''number of metaopochs'''
n_metaepochs=10


NNin1=2500 #dependent on Convu
NNout1=10

'''Convunet'''
Convu1_in=32
Convu2_in=16
Convu3_in=4
seed_convu=0

n_samples=150 #number of training samples
batch_size = 50
n_test=1000
n_training_epochs=3
print_distribution_data=False
std_modifier=0.05


Convu1_in=32
Convu2_in=16
Convu3_in=4



use_sigma_decay=True
sigma_start=1.0
sigma_goal=0.1

'''logging to screen variables'''
print_offsprings=True
print_distribution_data=False
use_sigma_decay=True #otherwise using constant sigma from config tab
sigma_start=1 
sigma_goal=0.05 #sigma goal after n_metaepochs
        
'''choose either method, softmax or elitist=keep only best offspring'''
use_softmax=True
temperature=0.05
use_elitist=False
use_winnerlist=False

n_metaepochs=30 #overwriting variable from config tab, delete later
n_offsprings=10 #overwriting variable from config tab, delete later

'''number of training epochs for Network2'''
n_offsp_epoch = 2
n_testing_epochs = 5
n_metaepochs=10





def custom_collate_fn(batch):
    transposed_data = list(zip(*batch))

    labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0])

    return imgs, labels


train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))
test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))






train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,
                                            transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                            batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)

test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,
                                          transform=torchvision.transforms.Compose([
                                          torchvision.transforms.ToTensor(),
                                          torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                          batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn,drop_last=True)



# test

#print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)

# optimization - loading the whole dataset into memory
train_images = jnp.array(train_dataset.data,dtype="float32").reshape(len(train_dataset), -1)
train_lbls = jnp.array(train_dataset.targets)



test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)

def save_p(obj, name,direc):
    path=os.path.join(direc,name)
    with open(path+".pkl", 'wb') as f:

        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
        f.close()

def softmax(results,temp):
    x = [z/temp for z in results]
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()
    
'''logging to txt and print'''
def logg (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    print(string_)
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    print(string_, array)
  
def pathandstuff():

    global save_txt
    global base_path
    global save_path

    if os.path.exists(local_path):
        '''Save running code file to log folder'''
        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook
        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder
        print("on local")
        base_path=local_path
    elif os.path.exists(googledrive_path):
        print("on google")
        base_path=googledrive_path
    else:
        raise ValueError('Please specify save path or connect to Google Drive')
        
    logs_path=base_path+"Logs/"
    '''Set logging and temp paths'''
    timestamp=time.strftime("%d.%m.%Y_%H.%M")
    foldername=timestamp
    save_path=os.path.join(logs_path,foldername)
    print("Log path:",save_path)
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    save_txt = os.path.join(save_path, 'Log_MNist_{}.txt'.format(foldername))
    
   
def plot_mean(mean_results,std_results): 
    mean=mean_results
    std=std_results
    x_axis=np.arange(0,len(mean), 1)

    plt.xticks(range(0,len(mean)))

    mean = np.array(mean)
    std = np.array(std)

    lower_bound = mean - std
    upper_bound = mean + std

    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3)
    plt.plot(mean,marker="o")
    plt.savefig(save_path+'/plot.png')
    return plt    

def sigma_decay(start, end, n_iter):
  return(end/start)**(1/n_iter)

def index_nhighest_list(n,list):
  return [i for x, i in heapq.nlargest(n,((x, i) for i, x in enumerate(list)))]

def matrix_distance(offspring_list):
    length=len(offspring_list)

    matrix=np.zeros((length, length))
    for c in range(length):
        for r in range(length):



            hilf=substract_two_dics(offspring_list[c],offspring_list[r])

            hilf2=square_dic(dict(hilf))
            sum_=dic_totalsum(hilf2)
            matrix[r][c]=sum_
    return matrix
    
def substract_two_dics(a,b):
    a = Counter(a)
    a.subtract(b)
    return a

'''logging to txt and print'''
def logg_to_file (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    
def log_variables():
    logg_to_file(("seed = {}".format(seed)))
    logg_to_file (("n_training_epochs = {}".format(n_training_epochs)))
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
    logg_to_file (("n_samples = {}".format(n_samples)))
    logg_to_file (("n_test = {}".format(n_test)))
    logg_to_file (("batch_size_train = {}".format(batch_size_train)))
    logg_to_file (("learning_rate = {}".format(learning_rate)))

    logg_to_file (("std_modifier = {}".format(std_modifier)))
    logg_to_file (("use_sigma_decay = {}".format(use_sigma_decay)))
    logg_to_file (("sigma_start = {}".format(sigma_start)))
    logg_to_file (("sigma_goal = {}".format(sigma_goal)))
    logg_to_file (("elitist = {}".format(elitist)))
    logg_to_file (("NNin1 = {}".format(NNin1)))
    logg_to_file (("NNout1 = {}".format(NNout1)))
    logg_to_file (("Convu_in1 = {}".format(Convu_in1)))
    logg_to_file (("Convu_out1 = {}".format(Convu_out1)))
    logg_to_file (("Convu_out2 = {}".format(Convu_out2)))

    logg_to_file (("kernelsize_ = {}".format(kernelsize_)))
    logg_to_file (("use_elitist = {}".format(use_elitist)))
    logg_to_file (("n_metaepochs = {}".format(n_metaepochs)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
                  
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_offsprings = {}".format(n_offsprings)))

    logg_to_file (("use_softmax = {}".format(use_softmax)))
    logg_to_file (("temperature = {}".format(temperature)))

                  
def square_dic(my_dict):
    my_dict.update((x, y**2) for x, y in my_dict.items())
    return my_dict   

def dic_totalsum(my_dict):
    return float(torch.sum(sum(sum(x) for x in my_dict.values())))

'''weighting every element in dic list with softmax weights from m'''
def weight_offspring_list(n_offsprings,offspring_list,m):
    for n in range(n_offsprings):

        for key in offspring_list[n]:    
            offspring_list[n][key] *=  m[n]
    return offspring_list

'''needed for softmax'''
def sum_list_of_dics(dic_list):
    c = Counter()
    for d in w_offspring_list:
        c.update(d)
    return c

'''get best performer from final resultlist[[0,2,0.8],[1,2,0.5],[2,2,0.2]'''
def best_index_bestperformer(lista):
    hilf=0
    index=None
    for i in range (len(lista)):
        if hilf<lista[i][2].item():
            index=i
            hilf=lista[i][2].item()
    return index

"""# Archiv"""

def accuracy_test(test_images,test_lbls):
  acc_test_images=test_images.reshape(-1,28,28)
  acc_test_images=acc_test_images[:, :,:,np.newaxis]
  #print(acc_test_images.shape)

  acc_test_images = conv_apply(weights, acc_test_images)
  #print(imgs.shape)
  acc_test_images = acc_test_images.reshape(*acc_test_images.shape[:1],-1) # Flatten


  #print("test_lbls",test_lbls.shape)
  #print("test_images",acc_test_images.shape)
  return accuracy(MLP_params, acc_test_images, test_lbls)