# -*- coding: utf-8 -*-
"""jax_vector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V4_9kdTf8sZ152W0MAgVNBpEpVsjMTVg

# MLP training on MNIST

1.   Testing on all 10000 images,because faster. Problem?
2.   Listeneintrag
"""

import numpy as np
import jax.numpy as jnp
from jax.scipy.special import logsumexp
import jax
from jax import jit, vmap, pmap, grad, value_and_grad
import random
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from jax.example_libraries import stax, optimizers
import torchvision
import torch
import torch.utils.data as data_utils
from jax import numpy as jnp
import time
import random

seed = 0
mnist_img_size = (28, 28)

def init_MLP(layer_widths, parent_key, scale=0.01):

    params = []
    keys = jax.random.split(parent_key, num=len(layer_widths)-1)

    for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):
        weight_key, bias_key = jax.random.split(key)
        params.append([
                       scale*jax.random.normal(weight_key, shape=(out_width, in_width)),
                       scale*jax.random.normal(bias_key, shape=(out_width,))
                       ]
        )

    return params

# test
key = jax.random.PRNGKey(seed)
MLP_params = init_MLP([2500, 10], key)
print(jax.tree_map(lambda x: x.shape, MLP_params))

def MLP_predict(params, x):
    hidden_layers = params[:-1]

    activation = x
    for w, b in hidden_layers:
        activation = jax.nn.relu(jnp.dot(w, activation) + b)

    w_last, b_last = params[-1]
    logits = jnp.dot(w_last, activation) + b_last

    # log(exp(o1)) - log(sum(exp(o1), exp(o2), ..., exp(o10)))
    # log( exp(o1) / sum(...) )
    return logits - logsumexp(logits)

# tests

# test single example


# test batched function
batched_MLP_predict = vmap(MLP_predict, in_axes=(None, 0))

dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))
print(dummy_imgs_flat.shape)
#predictions = batched_MLP_predict(MLP_params, dummy_imgs_flat)
#print(predictions.shape)

conv_init, conv_apply = stax.serial(
    stax.Conv(Convu1_in,(3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu2_in, (3,3), padding="SAME"),
    stax.BatchNorm(),
    stax.Relu,
    stax.MaxPool((2,2)),
    stax.Conv(Convu3_in, (3,3), padding="SAME"),
    stax.Relu,
    stax.MaxPool((2,2))
)




def create_offsprings(n_offspr, fath_weights):
  statedic_list=[]
  for i in range(0,n_offspr):
    dicta = [()] * len(fath_weights)
    for idx,w in enumerate(fath_weights):
        if w:
            w, b = w
            #print("Weights : {}, Biases : {}".format(w.shape, b.shape))
      
            '''if weight layer only contains 0 and 1, only copy original weight layer, dont add random noise. Purpose of these 0 and 1 layers unclear'''
            if any(w[0].shape==t for t in [(Convu1_in,) ,(Convu2_in,), (Convu3_in,)]):
              x_w=w
              x_b=b
            else:
              seed=np.random.randint(100000)
              key = jax.random.PRNGKey(seed)
              x_w = w+jax.random.normal(key,shape=w.shape)*std_modifier
              x_b = b+jax.random.normal(key,shape=b.shape)*std_modifier
            dicta[idx]=(x_w,x_b)
    statedic_list.append(jnp.array(dicta))
  return statedic_list

@jit
def loss_fn(params, imgs, gt_lbls):
  
    predictions = batched_MLP_predict(params, imgs)
    #print("predictions",predictions.shape)
    return -jnp.mean(predictions * gt_lbls)

'''Problems here!!!'''
@jit
def accuracy(weights,params, dataset_imgs, dataset_lbls):
    '''reshaping and adding axes, to adapt to code from internet, not pretty, but works'''

    acc_test_images=dataset_imgs.reshape(-1,28,28)
    acc_test_images=acc_test_images[:, :,:,np.newaxis]
    acc_test_images = conv_apply(weights, acc_test_images)
    acc_test_images = acc_test_images.reshape(*acc_test_images.shape[:1],-1) # Flatten
    pred_classes = jnp.argmax(batched_MLP_predict(params, acc_test_images), axis=1)
    return jnp.mean(dataset_lbls == pred_classes)

@jit
def update(params, imgs, gt_lbls, lr=0.01):
    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)

    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)

'''to do:
- imgs still in numpy arra
- check if weights stay the same'''
@jit
def train(key,conv_weights):
  
  # Create a MLP
  MLP_params = init_MLP([NNin1, NNout1], key)

  for epoch in range(n_training_epochs):
      
      for cnt, (imgs, lbls) in enumerate(train_loader):
          if cnt >= n_samples/(batch_size*n_training_epochs):
              break
          if print_distribution_data:
              print("label category counts:",np.unique(lbls, return_inverse=False, return_counts=True, axis=None)[1])
              
          

          gt_labels = jax.nn.one_hot(lbls, 10)
          
          
          
            
          imgs=np.moveaxis(imgs, 1, 3)
          
          imgs = conv_apply(conv_weights, imgs)
          
          imgs = imgs.reshape(*imgs.shape[:1],-1) # Flatten

          loss, MLP_params = jit_update(MLP_params, imgs, gt_labels)
          
          #if cnt % 50 == 0:
              #print(loss)
  from jax import random

  return loss,jit_accuracy(father_weights,MLP_params,test_images,test_lbls)
  
  #return loss,jit_accuracy(father_weights,MLP_params,test_images[random.randint(random.PRNGKey(0), (1000,), 0, 10000, dtype='uint8')],test_lbls[random.randint(random.PRNGKey(0), (1000,), 0, 10000, dtype='uint8')])

jit_accuracy=jit(accuracy)
jit_update=jit(update)
jit_train=jit(train)

jnp.shape(test_images)

jit_accuracy(father_weights,MLP_params,test_images,test_lbls)

'''Problems here!!!'''
@jit
def accuracy(conv_weights,MLP_params, dataset_imgs, dataset_lbls):
    '''reshaping and adding axes, to adapt to code from internet, not pretty, but works'''

    imgs = conv_apply(conv_weights, dataset_imgs.reshape(-1,28,28,1))
    print(jnp.shape(imgs))
    pred_classes = jnp.argmax(batched_MLP_predict(MLP_params, imgs.reshape(-1,2500)), axis=1)

    return jnp.mean(dataset_lbls == pred_classes)

@jit
def train(conv_weights, imgs, lbls,MLP_params ):
  
 
  for i in range(jnp.shape(imgs)[0]):
    
    gt_labels = jax.nn.one_hot(lbls[i], 10)
    affe = conv_apply(conv_weights, imgs[i].reshape(-1,28,28,1))
    loss, MLP_params = jit_update(MLP_params, affe.reshape(-1,2500), gt_labels)

    

  return MLP_params
  
  #return loss,jit_accuracy(father_weights,MLP_params,test_images[random.randint(random.PRNGKey(0), (1000,), 0, 10000, dtype='uint8')],test_lbls[random.randint(random.PRNGKey(0), (1000,), 0, 10000, dtype='uint8')])

train_img_off=train_images[random.randint(random.PRNGKey(0), (n_samples,), 0, 60000, dtype='uint8')]
train_lbl_off=train_lbls[random.randint(random.PRNGKey(0), (n_samples,), 0, 60000, dtype='uint8')]
train_img_off=train_img_off.reshape(int((n_samples/batch_size)),batch_size,28,28,1)
train_lbl_off=train_lbl_off.reshape(int((n_samples/batch_size)),batch_size)

'''print(jnp.shape(train_img_off)[0])
print(jnp.shape(train_lbl_off))
print(jnp.shape(train_lbl_off[0]))'''
affe=vmap_batched_train(father_weights, train_img_off,train_lbl_off,MLP_params)

def vmap_batched_train(father_weights, batch_affe, labelaffe,MLP_params ):
  return vmap(train, ( None, 1,1,None))(father_weights, batch_affe, labelaffe,MLP_params )

import jax.numpy as np
from jax import vmap
from jax import random



rng = jax.random.PRNGKey(123)

father_weights = conv_init(rng, (batch_size,28,28,1))
father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple

n_samples = 150
batch_size = 25
n_offsp_epoch=10

train_img_off=train_images[random.randint(random.PRNGKey(0), (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
train_lbl_off=train_lbls[random.randint(random.PRNGKey(0), (n_offsp_epoch*n_samples,), 0, 60000, dtype='uint8')]
train_img_off=train_img_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size,28,28,1)
train_lbl_off=train_lbl_off.reshape(n_offsp_epoch,int((n_samples/batch_size)),batch_size)

test_img_off=test_images[random.randint(random.PRNGKey(0), (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
test_lbl_off=test_lbls[random.randint(random.PRNGKey(0), (n_offsp_epoch*n_test,), 0, 10000, dtype='uint8')]
test_img_off=test_img_off.reshape(n_offsp_epoch,n_test,28,28,1)
test_lbl_off=test_lbl_off.reshape(n_offsp_epoch,n_test)

print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print(jnp.shape(test_img_off))
print(jnp.shape(test_lbl_off))

print("========================")
''''''
def bootstrapp_offspring(key,conv_weights, batch_affe, labelaffe,test_images,test_lbls):

  MLP_params = init_MLP([NNin1, NNout1], key)
  MLP_params_trained=train(conv_weights, batch_affe, labelaffe,MLP_params )
  #MLP_params_trained=vmap_batched_train(conv_weights, batch_affe, labelaffe,MLP_params) #vectorized training, for ex. 6*25 training examples. 
  print(jnp.shape(test_images))
  print(jnp.shape(test_lbls))
  print("jnp.shape(test_img_off)")
  
  result=jit_accuracy(father_weights,MLP_params_trained,test_images,test_lbls)
  return (result)

def vmap_bootstrapp_offspring(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):
  return vmap(bootstrapp_offspring, ( None,None, 0,0,0,0))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)

def vmap_bootstrapp_offspring(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls):
  return vmap(bootstrapp_offspring, ( None,None, 0,0,None,None))(key, conv_weights, batch_affe, labelaffe,test_images,test_lbls)

jnp.mean(vmap_bootstrapp_offspring(rng,father_weights,train_img_off,train_lbl_off,test_img_off,test_lbl_off))



MLP_params = init_MLP([NNin1, NNout1], key)

MLP_params=(train(key,father_weights, trainaffe, labelaffe,MLP_params ))

print(jit_accuracy(father_weights,MLP_params,test_images,test_lbls))

## Accuracy testing

test_img_off=test_images[random.randint(random.PRNGKey(0), (n_test,), 0, 10000, dtype='uint8')] #n_testing_epochs not implemented, only running one testing epoch per offspring epoch
test_lbl_off=test_lbls[random.randint(random.PRNGKey(0), (n_test,), 0, 10000, dtype='uint8')]
test_img_off=test_img_off.reshape(n_test,28,28,1)
test_lbl_off=test_lbl_off.reshape(n_test)

print(jnp.shape(train_img_off))
print(jnp.shape(train_lbl_off))
print(jnp.shape(test_img_off))
print(jnp.shape(test_lbl_off))

(10, 1000, 28, 28, 1)
(10, 1000)

imgs = conv_apply(father_weights, test_img_off.reshape(-1,28,28,1))
print(jnp.shape(imgs))
pred_classes = jnp.argmax(batched_MLP_predict(MLP_params, imgs.reshape(-1,2500)), axis=1)

print(jnp.mean(test_lbl_off == pred_classes))





n_metaepochs=100
n_offsprings=100
n_offsp_epoch=1

rng = jax.random.PRNGKey(123)

father_weights = conv_init(rng, (batch_size,28,28,1))
father_weights = father_weights[1] ## Weights are actually stored in second element of two value tuple

# Commented out IPython magic to ensure Python compatibility.
# %%time
# mean_results=[]
# std_results=[]
# best_performer=[]
# result_list_final=[]
# 
# #pathandstuff()
# #log_variables()
# 
# 
# 
# for m in range (n_metaepochs):
#     #print("Meta:",m)
#    
# 
#     metaseed=0+m #change seed and therefore data input for every metaepoch
#     torch.manual_seed(seed)
#     start_meta = time.time()
#     #logg("Metaepoch:{}".format(m))
# 
#     if m ==0:
#         rng = jax.random.PRNGKey(seed_convu)
#         father_weights = conv_init(rng, (batch_size,28,28,1))[1]
#         #offspring_list=create_offsprings(n_offsprings, father_weights)
# 
#     if m >1000:
#         if use_softmax==True:
#             results=np.array([float(x[2]) for x in result_list_final[m-1]])
#             softmax_vec = softmax(results, temperature)
#             print("Softmax_vec:", softmax_vec)
#             w_offspring_list=weight_offspring_list(n_offsprings,offspring_list,softmax_vec)
#             winner=dict(deepcopy(sum_list_of_dics(w_offspring_list)))
#             offspring_list=create_offsprings(n_offsprings, winner)
#             
#         
# 
#     result_list_metaepoch=[]
#     best_accur=0
#     #create_log()
#     #logg("Metaepoch:{}".format(n_metaepochs))
# 
#      
#     for offsp_count in range (0, n_offsprings):
#        
#         offsp_resulst_list=[]
#         for offsp_epoch in range (n_offsp_epoch):
# 
# 
#             #convu_weig=offspring_list[offsp_count]
#             key = jax.random.PRNGKey(seed_convu)
#             
#             loss, acc=jit_train(key,father_weights)
#             offsp_resulst_list.append([offsp_count,loss, acc])
#             #print([loss, acc])
# 
#         result_list_metaepoch.append(offsp_resulst_list)
# 
# 
# print(offsp_epoch_list)

offsp_epoch_list[0]

accuracy(conv_weights,MLP_params,test_images,test_lbls)

"""# **Testing**"""

test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)

from jax import random

random.randint(random.PRNGKey(0), (10000,), 0, 10000, dtype='uint8')

np.random.randint(10000, size=150)
a=(test_lbls)[np.random.randint(10000, size=150)]
a[0]

"""# **Funktions**"""

'''Set your file directorys'''

googledrive_path="/content/drive/MyDrive/Colab Notebooks/MNist/"
local_path="C:/Users/Flo/Documents/Uni/Masterarbeit/Hanabi/Mnist handwritten digits/"

'''set global seeds'''





'''number of training epochs for Network2'''
n_training_epochs = 3 #number of training epochs for every NN2.  
n_offsp_epoch = 10 #number of training and testing runs combined, to get an average for the performance of the Convu net.
n_testing_epochs = 3 # number of testing runs, per n_offsp_epoch

'''parameter Network2'''
n_samples = 150  #Number of training samples for NN2, distribution of data for 600: [68, 59, 66, 67, 47, 46, 65, 71, 53, 58])
n_test=1000 #Number of test samples for NN2. Needs to be multiples of batch_size_test=500 )
batch_size = 50 # for precise n_samples number must be: n_samples%batch_size_train=0

learning_rate = 0.1
momentum = 0.5
log_interval = 10



'''Standard deviation for gaussian noise in Network1'''
'''!!! Important hyperparameter, >=1 gives bad results'''
std_modifier=0.05


# !!! ATM Overriden in main code part
'''number of offsprings per metaepoch'''
n_offsprings=100
'''number of metaopochs'''
n_metaepochs=10


NNin1=2500 #dependent on Convu
NNout1=10

'''Convunet'''
Convu1_in=32
Convu2_in=16
Convu3_in=4
seed_convu=0

n_samples=150 #number of training samples
batch_size = 50
n_test=1000
n_training_epochs=3
print_distribution_data=False
std_modifier=0.05


Convu1_in=32
Convu2_in=16
Convu3_in=4



use_sigma_decay=True
sigma_start=1.0
sigma_goal=0.1

'''logging to screen variables'''
print_offsprings=True
print_distribution_data=False
use_sigma_decay=True #otherwise using constant sigma from config tab
sigma_start=1 
sigma_goal=0.05 #sigma goal after n_metaepochs
        
'''choose either method, softmax or elitist=keep only best offspring'''
use_softmax=True
temperature=0.05
use_elitist=False
use_winnerlist=False

n_metaepochs=30 #overwriting variable from config tab, delete later
n_offsprings=10 #overwriting variable from config tab, delete later

'''number of training epochs for Network2'''
n_offsp_epoch = 2
n_testing_epochs = 5
n_metaepochs=10





def custom_collate_fn(batch):
    transposed_data = list(zip(*batch))

    labels = np.array(transposed_data[1])
    imgs = np.stack(transposed_data[0])

    return imgs, labels


train_dataset = MNIST(root='train_mnist', train=True, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))
test_dataset = MNIST(root='test_mnist', train=False, download=True,transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))






train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,
                                            transform=torchvision.transforms.Compose([
                                            torchvision.transforms.ToTensor(),
                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                            batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)

test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,
                                          transform=torchvision.transforms.Compose([
                                          torchvision.transforms.ToTensor(),
                                          torchvision.transforms.Normalize((0.1307,), (0.3081,))])),
                                          batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn,drop_last=True)



# test

#print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)

# optimization - loading the whole dataset into memory
train_images = jnp.array(train_dataset.data,dtype="float32").reshape(len(train_dataset), -1)
train_lbls = jnp.array(train_dataset.targets)



test_images = jnp.array(test_dataset.data,dtype="float32").reshape(len(test_dataset), -1)
test_lbls = jnp.array(test_dataset.targets)

def save_p(obj, name,direc):
    path=os.path.join(direc,name)
    with open(path+".pkl", 'wb') as f:

        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)
        f.close()

def softmax(results,temp):
    x = [z/temp for z in results]
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()
    
'''logging to txt and print'''
def logg (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    print(string_)
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    print(string_, array)
  
def pathandstuff():

    global save_txt
    global base_path
    global save_path

    if os.path.exists(local_path):
        '''Save running code file to log folder'''
        #nb_full_path = os.path.join(os.getcwd(), nb_name) #path of current notebook
        #shutil.copy2(nb_full_path, save_path) #save running code file to log folder
        print("on local")
        base_path=local_path
    elif os.path.exists(googledrive_path):
        print("on google")
        base_path=googledrive_path
    else:
        raise ValueError('Please specify save path or connect to Google Drive')
        
    logs_path=base_path+"Logs/"
    '''Set logging and temp paths'''
    timestamp=time.strftime("%d.%m.%Y_%H.%M")
    foldername=timestamp
    save_path=os.path.join(logs_path,foldername)
    print("Log path:",save_path)
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    save_txt = os.path.join(save_path, 'Log_MNist_{}.txt'.format(foldername))
    
   
def plot_mean(mean_results,std_results): 
    mean=mean_results
    std=std_results
    x_axis=np.arange(0,len(mean), 1)

    plt.xticks(range(0,len(mean)))

    mean = np.array(mean)
    std = np.array(std)

    lower_bound = mean - std
    upper_bound = mean + std

    plt.fill_between(x_axis, lower_bound, upper_bound, alpha=.3)
    plt.plot(mean,marker="o")
    plt.savefig(save_path+'/plot.png')
    return plt    

def sigma_decay(start, end, n_iter):
  return(end/start)**(1/n_iter)

def index_nhighest_list(n,list):
  return [i for x, i in heapq.nlargest(n,((x, i) for i, x in enumerate(list)))]

def matrix_distance(offspring_list):
    length=len(offspring_list)

    matrix=np.zeros((length, length))
    for c in range(length):
        for r in range(length):



            hilf=substract_two_dics(offspring_list[c],offspring_list[r])

            hilf2=square_dic(dict(hilf))
            sum_=dic_totalsum(hilf2)
            matrix[r][c]=sum_
    return matrix
    
def substract_two_dics(a,b):
    a = Counter(a)
    a.subtract(b)
    return a

'''logging to txt and print'''
def logg_to_file (string_, array=None):
  if array is None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write("\n")
    file1.close()
    
  if array is not None:

    file1 = open(save_txt,"a")
    file1.write(string_)
    file1.write(str(array))
    file1.write("\n")
    file1.close()
    
def log_variables():
    logg_to_file(("seed = {}".format(seed)))
    logg_to_file (("n_training_epochs = {}".format(n_training_epochs)))
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
    logg_to_file (("n_samples = {}".format(n_samples)))
    logg_to_file (("n_test = {}".format(n_test)))
    logg_to_file (("batch_size_train = {}".format(batch_size_train)))
    logg_to_file (("learning_rate = {}".format(learning_rate)))

    logg_to_file (("std_modifier = {}".format(std_modifier)))
    logg_to_file (("use_sigma_decay = {}".format(use_sigma_decay)))
    logg_to_file (("sigma_start = {}".format(sigma_start)))
    logg_to_file (("sigma_goal = {}".format(sigma_goal)))
    logg_to_file (("elitist = {}".format(elitist)))
    logg_to_file (("NNin1 = {}".format(NNin1)))
    logg_to_file (("NNout1 = {}".format(NNout1)))
    logg_to_file (("Convu_in1 = {}".format(Convu_in1)))
    logg_to_file (("Convu_out1 = {}".format(Convu_out1)))
    logg_to_file (("Convu_out2 = {}".format(Convu_out2)))

    logg_to_file (("kernelsize_ = {}".format(kernelsize_)))
    logg_to_file (("use_elitist = {}".format(use_elitist)))
    logg_to_file (("n_metaepochs = {}".format(n_metaepochs)))
    logg_to_file (("n_testing_epochs = {}".format(n_testing_epochs)))
                  
    logg_to_file (("n_offsp_epoch = {}".format(n_offsp_epoch)))
    logg_to_file (("n_offsprings = {}".format(n_offsprings)))

    logg_to_file (("use_softmax = {}".format(use_softmax)))
    logg_to_file (("temperature = {}".format(temperature)))

                  
def square_dic(my_dict):
    my_dict.update((x, y**2) for x, y in my_dict.items())
    return my_dict   

def dic_totalsum(my_dict):
    return float(torch.sum(sum(sum(x) for x in my_dict.values())))

'''weighting every element in dic list with softmax weights from m'''
def weight_offspring_list(n_offsprings,offspring_list,m):
    for n in range(n_offsprings):

        for key in offspring_list[n]:    
            offspring_list[n][key] *=  m[n]
    return offspring_list

'''needed for softmax'''
def sum_list_of_dics(dic_list):
    c = Counter()
    for d in w_offspring_list:
        c.update(d)
    return c

'''get best performer from final resultlist[[0,2,0.8],[1,2,0.5],[2,2,0.2]'''
def best_index_bestperformer(lista):
    hilf=0
    index=None
    for i in range (len(lista)):
        if hilf<lista[i][2].item():
            index=i
            hilf=lista[i][2].item()
    return index

"""# Archiv"""

def accuracy_test(test_images,test_lbls):
  acc_test_images=test_images.reshape(-1,28,28)
  acc_test_images=acc_test_images[:, :,:,np.newaxis]
  #print(acc_test_images.shape)

  acc_test_images = conv_apply(weights, acc_test_images)
  #print(imgs.shape)
  acc_test_images = acc_test_images.reshape(*acc_test_images.shape[:1],-1) # Flatten


  #print("test_lbls",test_lbls.shape)
  #print("test_images",acc_test_images.shape)
  return accuracy(MLP_params, acc_test_images, test_lbls)